{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc5f882"
      },
      "source": [
        "## Install Data Augmentation Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install necessary Python libraries, such as `imgaug`, which will be used for performing various image transformations and augmenting COCO annotations (segmentation masks and bounding boxes) to create synthetic data. This ensures all required tools are available for the data generation process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d1d2da0"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to install necessary Python libraries for data augmentation. I will use `pip install` within a code block to install `imgaug`, `opencv-python`, and `scikit-image`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "386cec62",
        "outputId": "e9ea32bd-59cb-4aff-b275-2c5c1b52628a"
      },
      "source": [
        "pip install imgaug opencv-python scikit-image"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from imgaug) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imgaug) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from imgaug) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from imgaug) (3.10.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.37.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.1.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.6.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (2.9.0.post0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "424dae2b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the print statements within the `if imgs_count > 0:` block of cell `TPUKkd36GGgz` to improve phrasing and clarity. I will provide the full updated code for that cell, specifically focusing on the final success message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92f3476a",
        "outputId": "da61a1a4-f51f-4dc6-e41a-09f97f11a461"
      },
      "source": [
        "from google.colab import drive\n",
        "import os, json, shutil, zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# ================= MONTAR GOOGLE DRIVE =================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ================= CONFIGURACI√ìN =================\n",
        "# Ruta al ZIP en Google Drive\n",
        "zip_path = '/content/drive/MyDrive/alopecia_project/data/test-coco-upload.zip'  # ¬°AJUSTA ESTA RUTA!\n",
        "\n",
        "# Ruta donde quieres guardar el dataset YOLO en Drive\n",
        "output_dir = '/content/drive/MyDrive/alopecia_project/yolo_dataset'\n",
        "\n",
        "# ================= EXTRAER ZIP =================\n",
        "print(\"üì¶ Extrayendo ZIP de Google Drive...\")\n",
        "extract_dir = '/content/temp_extract'\n",
        "shutil.rmtree(extract_dir, ignore_errors=True)\n",
        "os.makedirs(extract_dir)\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Listar contenido\n",
        "        print(\"Contenido del ZIP:\")\n",
        "        file_list = zip_ref.namelist()\n",
        "        for file in file_list[:10]:  # Mostrar primeros 10\n",
        "            print(f\"  - {file}\")\n",
        "\n",
        "        if len(file_list) > 10:\n",
        "            print(f\"  ... y {len(file_list) - 10} m√°s\")\n",
        "\n",
        "        # Extraer TODO\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "    print(f\"‚úÖ ZIP extra√≠do en: {extract_dir}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå No se encontr√≥ el ZIP en: {zip_path}\")\n",
        "    print(\"\\nüìÅ Buscando archivos ZIP en Google Drive...\")\n",
        "\n",
        "    # Buscar archivos ZIP\n",
        "    import glob\n",
        "    zip_files = glob.glob('/content/drive/MyDrive/**/*.zip', recursive=True)\n",
        "    if zip_files:\n",
        "        print(\"Archivos ZIP encontrados:\")\n",
        "        for zf in zip_files[:5]:\n",
        "            print(f\"  - {zf}\")\n",
        "        zip_path = zip_files[0]\n",
        "        print(f\"\\n‚úÖ Usando: {zip_path}\")\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No se encontr√≥ ning√∫n archivo ZIP\")\n",
        "\n",
        "# ================= BUSCAR ARCHIVOS =================\n",
        "# Buscar JSON\n",
        "json_files = list(Path(extract_dir).rglob('*.json'))\n",
        "if not json_files:\n",
        "    raise FileNotFoundError(\"‚ùå No se encontr√≥ JSON en el ZIP\")\n",
        "\n",
        "json_path = str(json_files[0])\n",
        "print(f\"‚úÖ JSON encontrado: {json_path}\")\n",
        "\n",
        "# Buscar carpeta de im√°genes\n",
        "images_dir = None\n",
        "possible_dirs = ['images', 'img', 'upload', 'data', 'media']\n",
        "for dir_name in possible_dirs:\n",
        "    dir_path = Path(extract_dir) / dir_name\n",
        "    if dir_path.exists() and any(dir_path.iterdir()):\n",
        "        images_dir = dir_path\n",
        "        print(f\"‚úÖ Carpeta de im√°genes encontrada: {images_dir}\")\n",
        "        break\n",
        "\n",
        "# Si no, buscar cualquier carpeta con im√°genes\n",
        "if not images_dir:\n",
        "    for root, dirs, files in os.walk(extract_dir):\n",
        "        if any(f.lower().endswith(('.jpg', '.png', '.jpeg')) for f in files):\n",
        "            images_dir = Path(root)\n",
        "            print(f\"‚úÖ Im√°genes encontradas en: {images_dir}\")\n",
        "            break\n",
        "\n",
        "if not images_dir:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ carpeta de im√°genes espec√≠fica, buscando en todo el extract...\")\n",
        "    images_dir = Path(extract_dir)\n",
        "\n",
        "# Listar im√°genes encontradas\n",
        "imagenes = []\n",
        "for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.PNG']:\n",
        "    imagenes.extend(list(images_dir.rglob(ext)))\n",
        "\n",
        "print(f\"üì∏ Total im√°genes encontradas: {len(imagenes)}\")\n",
        "for img in imagenes[:5]:\n",
        "    print(f\"  - {img.name}\")\n",
        "\n",
        "# ================= CARGAR JSON =================\n",
        "with open(json_path, 'r') as f:\n",
        "    coco_data = json.load(f) # Renamed 'datos' to 'coco_data' to reflect COCO format\n",
        "\n",
        "print(f\"\\nüìä Total de im√°genes en JSON (COCO): {len(coco_data.get('images', []))}\")\n",
        "print(f\"üìä Total de anotaciones en JSON (COCO): {len(coco_data.get('annotations', []))}\")\n",
        "\n",
        "# Mostrar estructura del JSON para depuraci√≥n\n",
        "if coco_data and isinstance(coco_data, dict):\n",
        "    print(\"\\nüìù Estructura del JSON (claves principales):\")\n",
        "    for key in coco_data.keys():\n",
        "        print(f\"  - {key}\")\n",
        "    if 'images' in coco_data and coco_data['images']:\n",
        "        print(\"\\nüìù Estructura del primer elemento de 'images':\")\n",
        "        for key in list(coco_data['images'][0].keys())[:10]:\n",
        "            print(f\"  - {key}\")\n",
        "    if 'annotations' in coco_data and coco_data['annotations']:\n",
        "        print(\"\\nüìù Estructura del primer elemento de 'annotations':\")\n",
        "        for key in list(coco_data['annotations'][0].keys())[:10]:\n",
        "            print(f\"  - {key}\")\n",
        "\n",
        "# ================= CREAR ESTRUCTURA YOLO =================\n",
        "yolo_dir = Path(output_dir)\n",
        "shutil.rmtree(yolo_dir, ignore_errors=True)  # Limpiar si existe\n",
        "(yolo_dir / 'images').mkdir(parents=True, exist_ok=True)\n",
        "(yolo_dir / 'labels').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Mapear categor√≠as a IDs de clase y almacenar nombres de clase\n",
        "class_names = [cat['name'] for cat in coco_data.get('categories', [])]\n",
        "class_id_map = {cat['id']: i for i, cat in enumerate(coco_data.get('categories', []))}\n",
        "\n",
        "# Mapear image_id a informaci√≥n de imagen para b√∫squeda r√°pida\n",
        "image_info_map = {img['id']: img for img in coco_data.get('images', [])}\n",
        "\n",
        "procesadas = 0\n",
        "imagenes_usadas = set()\n",
        "\n",
        "# Iterar sobre cada imagen en el JSON COCO\n",
        "for i, img_data in enumerate(coco_data.get('images', [])):\n",
        "    image_id = img_data['id']\n",
        "    nombre_imagen_json = img_data.get('file_name')\n",
        "    img_width = img_data.get('width')\n",
        "    img_height = img_data.get('height')\n",
        "\n",
        "    if not nombre_imagen_json or img_width is None or img_height is None:\n",
        "        print(f\"‚ùå Informaci√≥n incompleta para imagen ID {image_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n--- Procesando imagen {i+1}/{len(coco_data['images'])} (ID: {image_id}) ---\")\n",
        "\n",
        "    # Limpiar nombre (quitar rutas, par√°metros URL, etc.)\n",
        "    nombre_limpio = os.path.basename(str(nombre_imagen_json))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    print(f\"üîç Buscando imagen: '{nombre_limpio}'\")\n",
        "\n",
        "    imagen_encontrada = None\n",
        "    # Prioridad: Coincidencia exacta con nombre de archivo\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            imagen_encontrada = img_path_candidate\n",
        "            break\n",
        "    # Si no se encuentra, intentar por nombre base sin extensi√≥n\n",
        "    if not imagen_encontrada:\n",
        "        nombre_base = os.path.splitext(nombre_limpio)[0]\n",
        "        for img_path_candidate in imagenes:\n",
        "            if os.path.splitext(img_path_candidate.name)[0] == nombre_base:\n",
        "                imagen_encontrada = img_path_candidate\n",
        "                break\n",
        "    # Si a√∫n no se encuentra, intentar coincidencia parcial con nombres limpios\n",
        "    if not imagen_encontrada:\n",
        "        nombre_limpio_lower = nombre_limpio.lower()\n",
        "        for img_path_candidate in imagenes:\n",
        "            img_name_lower = img_path_candidate.name.lower()\n",
        "            if nombre_limpio_lower in img_name_lower or img_name_lower in img_path_candidate.name.lower():\n",
        "                imagen_encontrada = img_path_candidate\n",
        "                break\n",
        "            if '-' in nombre_limpio_lower: # Handle hash prefixes (e.g., \"c6483cef-\")\n",
        "                nombre_sin_hash = nombre_limpio_lower.split('-', 1)[1]\n",
        "                if nombre_sin_hash in img_name_lower:\n",
        "                    imagen_encontrada = img_path_candidate\n",
        "                    break\n",
        "\n",
        "    if imagen_encontrada: # Proceed only if image file is found\n",
        "        if imagen_encontrada in imagenes_usadas:\n",
        "            print(f\"‚ö†Ô∏è  Imagen ya procesada (posible duplicado en JSON o nombres): {imagen_encontrada.name}\")\n",
        "            continue # Skip to avoid creating duplicate YOLO entries\n",
        "\n",
        "        print(f\"‚úÖ Imagen encontrada: {imagen_encontrada.name}\")\n",
        "        imagenes_usadas.add(imagen_encontrada)\n",
        "\n",
        "        # 3. Copiar imagen\n",
        "        ext = imagen_encontrada.suffix\n",
        "        img_dest = yolo_dir / 'images' / f\"{image_id}{ext}\" # Use COCO image ID as filename\n",
        "        shutil.copy(imagen_encontrada, img_dest)\n",
        "\n",
        "        # 4. Crear etiquetas YOLO\n",
        "        txt_path = yolo_dir / 'labels' / f\"{image_id}.txt\"\n",
        "\n",
        "        with open(txt_path, 'w') as f_txt:\n",
        "            anotaciones_escritas = 0\n",
        "            # Filtrar anotaciones para esta imagen\n",
        "            image_annotations = [ann for ann in coco_data.get('annotations', []) if ann['image_id'] == image_id]\n",
        "\n",
        "            for ann in image_annotations:\n",
        "                if 'bbox' in ann: # Process bounding box annotations\n",
        "                    x_min, y_min, ann_width, ann_height = ann['bbox']\n",
        "\n",
        "                    # Convert to normalized YOLO format\n",
        "                    center_x = (x_min + ann_width / 2) / img_width\n",
        "                    center_y = (y_min + ann_height / 2) / img_height\n",
        "                    norm_width = ann_width / img_width\n",
        "                    norm_height = ann_height / img_height\n",
        "\n",
        "                    category_id = ann.get('category_id')\n",
        "                    if category_id is None or category_id not in class_id_map:\n",
        "                        print(f\"   ‚ùå Categor√≠a ID {category_id} no encontrada en 'categories'. Saltando anotaci√≥n.\")\n",
        "                        continue\n",
        "\n",
        "                    class_idx = class_id_map[category_id] # YOLO class index\n",
        "\n",
        "                    f_txt.write(f\"{class_idx} {center_x:.6f} {center_y:.6f} {norm_width:.6f} {norm_height:.6f}\\n\")\n",
        "                    anotaciones_escritas += 1\n",
        "                    class_name_display = class_names[class_idx] if class_idx < len(class_names) else f\"ID {category_id}\"\n",
        "                    print(f\"   üéØ Anotaci√≥n BBOX: {class_name_display} - ({center_x:.2f}, {center_y:.2f}, {norm_width:.2f}, {norm_height:.2f})\")\n",
        "                elif 'segmentation' in ann and ann['segmentation']:\n",
        "                    print(f\"   ‚ö†Ô∏è Se encontr√≥ anotaci√≥n de segmentaci√≥n para ID {image_id}, pero el script actual solo procesa 'bbox' a YOLO.\")\n",
        "\n",
        "            if anotaciones_escritas == 0:\n",
        "                print(f\"‚ö†Ô∏è  No se encontraron anotaciones BBOX v√°lidas para esta imagen\")\n",
        "                # Create an empty label file if no valid annotations were processed\n",
        "                with open(txt_path, 'w') as f_txt:\n",
        "                    f_txt.write(\"\")\n",
        "\n",
        "        procesadas += 1\n",
        "    else:\n",
        "        print(f\"‚ùå Imagen NO encontrada en el directorio de extracci√≥n para '{nombre_limpio}'\")\n",
        "\n",
        "# ================= CREAR dataset.yaml =================\n",
        "with open(yolo_dir / 'dataset.yaml', 'w') as f:\n",
        "    f.write(f\"# Dataset Alopecia - YOLO\\n\")\n",
        "    f.write(f\"path: {output_dir}\\n\")\n",
        "    f.write(f\"train: images\\n\")\n",
        "    f.write(f\"val: images\\n\\n\")\n",
        "    f.write(f\"# Clases\\n\")\n",
        "    f.write(f\"nc: {len(class_names)}\\n\")\n",
        "    f.write(f\"names: {class_names}\\n\")\n",
        "\n",
        "# ================= RESUMEN =================\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"üéâ PROCESO COMPLETADO\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"‚úÖ Im√°genes en JSON (COCO): {len(coco_data.get('images', []))}\")\n",
        "print(f\"‚úÖ Anotaciones en JSON (COCO): {len(coco_data.get('annotations', []))}\")\n",
        "print(f\"‚úÖ Im√°genes procesadas y copiadas: {procesadas}\")\n",
        "print(f\"‚úÖ Clases encontradas: {class_names}\")\n",
        "print(f\"‚úÖ Total clases: {len(class_names)}\")\n",
        "\n",
        "# Estad√≠sticas\n",
        "imgs_count = len(list((yolo_dir / 'images').glob('*')))\n",
        "labels_count = len(list((yolo_dir / 'labels').glob('*.txt')))\n",
        "\n",
        "print(f\"\\nüìä Estad√≠sticas finales:\")\n",
        "print(f\"   Im√°genes copiadas a YOLO: {imgs_count}\")\n",
        "print(f\"   Etiquetas creadas en YOLO: {labels_count}\")\n",
        "\n",
        "# Mostrar ubicaci√≥n final\n",
        "print(f\"\\nüìÅ Dataset YOLO creado en Google Drive:\")\n",
        "print(f\"   {output_dir}/\")\n",
        "print(f\"   ‚îú‚îÄ‚îÄ images/\")\n",
        "print(f\"   ‚îú‚îÄ‚îÄ labels/\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ dataset.yaml\")\n",
        "\n",
        "# Verificar acceso\n",
        "if imgs_count > 0:\n",
        "    print(f\"\\n‚úÖ El dataset YOLO ha sido preparado con √©xito y est√° listo para el entrenamiento.\")\n",
        "    print(f\"   Aseg√∫rate de que los archivos se encuentren en las siguientes ubicaciones:\")\n",
        "    print(f\"   - Ruta del dataset: {output_dir}\")\n",
        "    print(f\"   - Im√°genes de entrenamiento: {imgs_count} archivos en {output_dir}/images/\")\n",
        "    print(f\"   - Archivos de etiquetas (bounding boxes): {labels_count} archivos en {output_dir}/labels/\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  No se procesaron im√°genes para el dataset YOLO. Por favor, verifica:\")\n",
        "    print(f\"   - Que el ZIP contenga im√°genes y anotaciones COCO v√°lidas con 'bbox'.\")\n",
        "    print(f\"   - Que los nombres de las im√°genes en el COCO JSON coincidan con los archivos de imagen extra√≠dos.\")\n",
        "\n",
        "# Limpiar temporal\n",
        "# shutil.rmtree(extract_dir, ignore_errors=True) # Commented out to prevent premature deletion\n",
        "print(f\"\\nüßπ Directorio temporal conservado para procesamiento posterior.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üì¶ Extrayendo ZIP de Google Drive...\n",
            "Contenido del ZIP:\n",
            "  - test-coco/\n",
            "  - test-coco/images/\n",
            "  - test-coco/images/133e41f0-imgi_460_alopecia.jpg\n",
            "  - test-coco/images/26aee95b-imgi_454_areata5.jpg\n",
            "  - test-coco/images/3022b173-imgi_400_Hair-loss_SP-Feature.jpg\n",
            "  - test-coco/images/769446b8-imgi_413_alopecia-androgenica-femenina-mujer-joven.jpg\n",
            "  - test-coco/images/79b0bbb7-imgi_429_alopecia-areata.jpg.webp\n",
            "  - test-coco/images/7a30afe5-imgi_188_persona-con-alopecia.jpg\n",
            "  - test-coco/images/7de3a7df-imgi_109_images.jpg\n",
            "  - test-coco/images/7e88f071-imgi_404_Alopecia-en-adolescentes.jpg\n",
            "  ... y 8 m√°s\n",
            "‚úÖ ZIP extra√≠do en: /content/temp_extract\n",
            "‚úÖ JSON encontrado: /content/temp_extract/test-coco/result_coco.json\n",
            "‚úÖ Im√°genes encontradas en: /content/temp_extract/test-coco/images\n",
            "üì∏ Total im√°genes encontradas: 15\n",
            "  - 7a30afe5-imgi_188_persona-con-alopecia.jpg\n",
            "  - 7de3a7df-imgi_109_images.jpg\n",
            "  - f5c645c5-imgi_444_1205353.jpg\n",
            "  - 769446b8-imgi_413_alopecia-androgenica-femenina-mujer-joven.jpg\n",
            "  - 3022b173-imgi_400_Hair-loss_SP-Feature.jpg\n",
            "\n",
            "üìä Total de im√°genes en JSON (COCO): 12\n",
            "üìä Total de anotaciones en JSON (COCO): 19\n",
            "\n",
            "üìù Estructura del JSON (claves principales):\n",
            "  - images\n",
            "  - annotations\n",
            "  - categories\n",
            "  - info\n",
            "\n",
            "üìù Estructura del primer elemento de 'images':\n",
            "  - id\n",
            "  - file_name\n",
            "  - path\n",
            "  - width\n",
            "  - height\n",
            "\n",
            "üìù Estructura del primer elemento de 'annotations':\n",
            "  - id\n",
            "  - image_id\n",
            "  - category_id\n",
            "  - segmentation\n",
            "  - area\n",
            "  - bbox\n",
            "  - iscrowd\n",
            "  - annotator\n",
            "\n",
            "--- Procesando imagen 1/12 (ID: 0) ---\n",
            "üîç Buscando imagen: 'c6483cef-imgi_420_3b5117126280-ni-o-con-alopecia.jpg'\n",
            "‚úÖ Imagen encontrada: c6483cef-imgi_420_3b5117126280-ni-o-con-alopecia.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.59, 0.31, 0.13, 0.39)\n",
            "\n",
            "--- Procesando imagen 2/12 (ID: 1) ---\n",
            "üîç Buscando imagen: 'dfc0877a-imgi_395_2022-n1-5_Figura1.jpg'\n",
            "‚úÖ Imagen encontrada: dfc0877a-imgi_395_2022-n1-5_Figura1.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.57, 0.44, 0.23, 0.21)\n",
            "\n",
            "--- Procesando imagen 3/12 (ID: 2) ---\n",
            "üîç Buscando imagen: '133e41f0-imgi_460_alopecia.jpg'\n",
            "‚úÖ Imagen encontrada: 133e41f0-imgi_460_alopecia.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.61, 0.47, 0.29, 0.47)\n",
            "\n",
            "--- Procesando imagen 4/12 (ID: 4) ---\n",
            "üîç Buscando imagen: '7e88f071-imgi_404_Alopecia-en-adolescentes.jpg'\n",
            "‚úÖ Imagen encontrada: 7e88f071-imgi_404_Alopecia-en-adolescentes.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.58, 0.43, 0.49, 0.68)\n",
            "\n",
            "--- Procesando imagen 5/12 (ID: 5) ---\n",
            "üîç Buscando imagen: 'ab87ea98-imgi_398_alopecia-00.jpg'\n",
            "‚úÖ Imagen encontrada: ab87ea98-imgi_398_alopecia-00.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.53, 0.48, 0.35, 0.55)\n",
            "\n",
            "--- Procesando imagen 6/12 (ID: 6) ---\n",
            "üîç Buscando imagen: '769446b8-imgi_413_alopecia-androgenica-femenina-mujer-joven.jpg'\n",
            "‚úÖ Imagen encontrada: 769446b8-imgi_413_alopecia-androgenica-femenina-mujer-joven.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.59, 0.54, 0.44, 0.42)\n",
            "\n",
            "--- Procesando imagen 7/12 (ID: 7) ---\n",
            "üîç Buscando imagen: '26aee95b-imgi_454_areata5.jpg'\n",
            "‚úÖ Imagen encontrada: 26aee95b-imgi_454_areata5.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.44, 0.58, 0.19, 0.23)\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.59, 0.36, 0.79, 0.54)\n",
            "\n",
            "--- Procesando imagen 8/12 (ID: 9) ---\n",
            "üîç Buscando imagen: 'f5c645c5-imgi_444_1205353.jpg'\n",
            "‚úÖ Imagen encontrada: f5c645c5-imgi_444_1205353.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.57, 0.69, 0.40, 0.56)\n",
            "\n",
            "--- Procesando imagen 9/12 (ID: 10) ---\n",
            "üîç Buscando imagen: 'c58a0df2-imgi_408_576px-Alopecia_areata_bald_spot.jpg'\n",
            "‚úÖ Imagen encontrada: c58a0df2-imgi_408_576px-Alopecia_areata_bald_spot.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.47, 0.52, 0.12, 0.11)\n",
            "\n",
            "--- Procesando imagen 10/12 (ID: 11) ---\n",
            "üîç Buscando imagen: 'b046c7b8-imgi_480_un-joven-que-se-ve-la-preocupaci%C3%B3n-del-espejo-de-calvicie-con_WecBAMh.jpg'\n",
            "‚úÖ Imagen encontrada: 7a30afe5-imgi_188_persona-con-alopecia.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.63, 0.45, 0.52, 0.55)\n",
            "\n",
            "--- Procesando imagen 11/12 (ID: 12) ---\n",
            "üîç Buscando imagen: '3022b173-imgi_400_Hair-loss_SP-Feature.jpg'\n",
            "‚úÖ Imagen encontrada: 3022b173-imgi_400_Hair-loss_SP-Feature.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.47, 0.57, 0.81, 0.81)\n",
            "\n",
            "--- Procesando imagen 12/12 (ID: 13) ---\n",
            "üîç Buscando imagen: '7de3a7df-imgi_109_images.jpg'\n",
            "‚úÖ Imagen encontrada: 7de3a7df-imgi_109_images.jpg\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.25, 0.57, 0.25, 0.26)\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.82, 0.50, 0.25, 0.26)\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.72, 0.32, 0.10, 0.09)\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.19, 0.35, 0.12, 0.23)\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.64, 0.23, 0.12, 0.08)\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.49, 0.38, 0.31, 0.38)\n",
            "   üéØ Anotaci√≥n BBOX: Alopecia - (0.16, 0.37, 0.33, 0.37)\n",
            "\n",
            "==================================================\n",
            "üéâ PROCESO COMPLETADO\n",
            "==================================================\n",
            "‚úÖ Im√°genes en JSON (COCO): 12\n",
            "‚úÖ Anotaciones en JSON (COCO): 19\n",
            "‚úÖ Im√°genes procesadas y copiadas: 12\n",
            "‚úÖ Clases encontradas: ['Alopecia']\n",
            "‚úÖ Total clases: 1\n",
            "\n",
            "üìä Estad√≠sticas finales:\n",
            "   Im√°genes copiadas a YOLO: 12\n",
            "   Etiquetas creadas en YOLO: 12\n",
            "\n",
            "üìÅ Dataset YOLO creado en Google Drive:\n",
            "   /content/drive/MyDrive/alopecia_project/yolo_dataset/\n",
            "   ‚îú‚îÄ‚îÄ images/\n",
            "   ‚îú‚îÄ‚îÄ labels/\n",
            "   ‚îî‚îÄ‚îÄ dataset.yaml\n",
            "\n",
            "‚úÖ El dataset YOLO ha sido preparado con √©xito y est√° listo para el entrenamiento.\n",
            "   Aseg√∫rate de que los archivos se encuentren en las siguientes ubicaciones:\n",
            "   - Ruta del dataset: /content/drive/MyDrive/alopecia_project/yolo_dataset\n",
            "   - Im√°genes de entrenamiento: 12 archivos en /content/drive/MyDrive/alopecia_project/yolo_dataset/images/\n",
            "   - Archivos de etiquetas (bounding boxes): 12 archivos en /content/drive/MyDrive/alopecia_project/yolo_dataset/labels/\n",
            "\n",
            "üßπ Directorio temporal conservado para procesamiento posterior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39483a26"
      },
      "source": [
        "### Confirmation of Updated Message\n",
        "\n",
        "The updated output message in the code cell `TPUKkd36GGgz` is now clear and accurately reflects the dataset's readiness for YOLO training.\n",
        "\n",
        "The improvements include:\n",
        "1.  **Clearer affirmative statement**: \"‚úÖ El dataset YOLO ha sido preparado con √©xito y est√° listo para el entrenamiento.\" (The YOLO dataset has been successfully prepared and is ready for training.)\n",
        "2.  **Explicit instruction**: \"Aseg√∫rate de que los archivos se encuentren en las siguientes ubicaciones:\" (Ensure that the files are in the following locations:)\n",
        "3.  **Specific details**: The message now clearly itemizes the path to the dataset, the exact count and location of the training images, and the exact count and location of the bounding box label files. This provides comprehensive information to the user, ensuring they know precisely where to find their prepared dataset components for the next training steps.\n",
        "\n",
        "This level of detail is crucial for usability and reduces ambiguity when proceeding with YOLO model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e456c9b4"
      },
      "source": [
        "## Generate Augmented Dataset\n",
        "\n",
        "### Subtask:\n",
        "Generate approximately 100 synthetic images and their corresponding COCO annotations by applying various geometric and photometric transformations to the initial 10 images. Ensure COCO annotations (segmentation polygons) are transformed consistently with the images. Save the augmented data in a new directory structure, similar to the `aligned_dataset`, for the segmentation model's training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fd26dce"
      },
      "source": [
        "**Reasoning**:\n",
        "The first instruction is to define the input and output directories and initialize the `imgaug` augmentation pipeline. I also need to load the `coco_data` and `imagenes` from the previous steps to begin the augmentation process. The augmentation sequence will include geometric and photometric transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "collapsed": true,
        "id": "8a4c489d",
        "outputId": "ac6ef197-10ab-4834-d0f2-acaebbfc95ca"
      },
      "source": [
        "print('üîÑ Downgrading numpy for imgaug compatibility...')\n",
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.26.4\n",
        "print('‚úÖ numpy downgraded to 1.26.4.')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Downgrading numpy for imgaug compatibility...\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "59c0a4f70a844a9f8b3cad156c1fa013"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ numpy downgraded to 1.26.4.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "832fb4ef",
        "outputId": "63d047ee-d5df-492d-e9d1-884453f07300"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "\n",
        "# ================= 1. DEFINIR DIRECTORIOS Y CONFIGURACI√ìN =================\n",
        "# Input: Directorio de extracci√≥n temporal del ZIP y JSON COCO original\n",
        "# `extract_dir` y `json_path` deben venir de la celda de extracci√≥n (TPUKkd36GGgz)\n",
        "# `coco_data` y `imagenes` tambi√©n de la celda TPUKkd36GGgz\n",
        "# `class_names` de la celda TPUKkd36GGgz\n",
        "\n",
        "# Output: Directorio para el dataset aumentado\n",
        "augmented_output_dir = Path('/content/alopecia_dataset/augmented_dataset')\n",
        "augmented_img_dir = augmented_output_dir / 'images'\n",
        "augmented_annot_dir = augmented_output_dir / 'annotations'\n",
        "\n",
        "# Limpiar y crear directorios\n",
        "shutil.rmtree(augmented_output_dir, ignore_errors=True)\n",
        "augmented_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "augmented_annot_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Directorios para el dataset aumentado creados en: {augmented_output_dir}\")\n",
        "\n",
        "# ================= 2. INICIALIZAR AUMENTADORES imgaug =================\n",
        "# Definir la secuencia de aumentaci√≥n. Usamos `random_order=True` para variar el orden.\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5), # Volteo horizontal con 50% de probabilidad\n",
        "    iaa.Affine(\n",
        "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # Escala 80-120%\n",
        "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # Traslaci√≥n -20% a +20%\n",
        "        rotate=(-25, 25), # Rotaci√≥n -25 a +25 grados\n",
        "        shear=(-8, 8) # Cizallamiento -8 a +8 grados\n",
        "    ),\n",
        "    iaa.Multiply((0.8, 1.2), per_channel=0.2), # Ajuste de brillo (80-120%)\n",
        "    iaa.LinearContrast((0.75, 1.5)), # Ajuste de contraste\n",
        "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # Ruido Gaussiano\n",
        "    iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 1.5))), # Afilado\n",
        "    iaa.Sometimes(0.2, iaa.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))), # Relieve\n",
        "    iaa.Sometimes(0.2, iaa.EdgeDetect(alpha=(0.0, 0.7))), # Detecci√≥n de bordes\n",
        "    iaa.Sometimes(0.2, iaa.MotionBlur(k=(3, 7), angle=[-45, 45])), # Desenfoque de movimiento\n",
        "    iaa.Sometimes(0.2, iaa.AddToHueAndSaturation((-20, 20))) # Cambios de tono y saturaci√≥n\n",
        "], random_order=True) # Aplicar aumentaciones en orden aleatorio\n",
        "\n",
        "print(\"‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\")\n",
        "\n",
        "# ================= 3. & 4. CARGAR COCO ORIGINAL Y MAPPING =================\n",
        "# `coco_data` y `imagenes` ya est√°n disponibles desde la ejecuci√≥n anterior.\n",
        "# Asegur√©monos de tener un mapa de image_id a info de imagen y anotaciones\n",
        "\n",
        "original_images_info = {img['id']: img for img in coco_data.get('images', [])}\n",
        "original_annotations_info = {}\n",
        "for ann in coco_data.get('annotations', []):\n",
        "    image_id = ann['image_id']\n",
        "    if image_id not in original_annotations_info:\n",
        "        original_annotations_info[image_id] = []\n",
        "    original_annotations_info[image_id].append(ann)\n",
        "\n",
        "print(f\"‚úÖ COCO original cargado. Im√°genes: {len(original_images_info)}, Anotaciones para {len(original_annotations_info)} im√°genes.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Directorios para el dataset aumentado creados en: /content/alopecia_dataset/augmented_dataset\n",
            "‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'coco_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1365339115.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Asegur√©monos de tener un mapa de image_id a info de imagen y anotaciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0moriginal_images_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoco_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0moriginal_annotations_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoco_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'coco_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf48e29c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `imgaug` is not compatible with `numpy` version 2.0 or newer, as indicated by the `AttributeError: np.sctypes` was removed. I need to downgrade `numpy` to a compatible version (e.g., `1.26.4`) before `imgaug` can be used. This will be done in a new cell, followed by the original code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afb946d9"
      },
      "source": [
        "**Reasoning**:\n",
        "Numpy has been downgraded, so now I will re-run the code to initialize `imgaug` and set up the directories and mappings for the augmented dataset. This should now execute without the `AttributeError`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "2115df1a",
        "outputId": "25d6073b-7cb5-4184-9ab5-afec22ff96da"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "\n",
        "# ================= 1. DEFINIR DIRECTORIOS Y CONFIGURACI√ìN =================\n",
        "# Input: Directorio de extracci√≥n temporal del ZIP y JSON COCO original\n",
        "# `extract_dir` y `json_path` deben venir de la celda de extracci√≥n (TPUKkd36GGgz)\n",
        "# `coco_data` y `imagenes` tambi√©n de la celda TPUKkd36GGgz\n",
        "# `class_names` de la celda TPUKkd36GGgz\n",
        "\n",
        "# Output: Directorio para el dataset aumentado\n",
        "augmented_output_dir = Path('/content/alopecia_dataset/augmented_dataset')\n",
        "augmented_img_dir = augmented_output_dir / 'images'\n",
        "augmented_annot_dir = augmented_output_dir / 'annotations'\n",
        "\n",
        "# Limpiar y crear directorios\n",
        "shutil.rmtree(augmented_output_dir, ignore_errors=True)\n",
        "augmented_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "augmented_annot_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Directorios para el dataset aumentado creados en: {augmented_output_dir}\")\n",
        "\n",
        "# ================= 2. INICIALIZAR AUMENTADORES imgaug =================\n",
        "# Definir la secuencia de aumentaci√≥n. Usamos `random_order=True` para variar el orden.\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5), # Volteo horizontal con 50% de probabilidad\n",
        "    iaa.Affine(\n",
        "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # Escala 80-120%\n",
        "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # Traslaci√≥n -20% a +20%\n",
        "        rotate=(-25, 25), # Rotaci√≥n -25 a +25 grados\n",
        "        shear=(-8, 8) # Cizallamiento -8 a +8 grados\n",
        "    ),\n",
        "    iaa.Multiply((0.8, 1.2), per_channel=0.2), # Ajuste de brillo (80-120%)\n",
        "    iaa.LinearContrast((0.75, 1.5)), # Ajuste de contraste\n",
        "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # Ruido Gaussiano\n",
        "    iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 1.5))), # Afilado\n",
        "    iaa.Sometimes(0.2, iaa.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))), # Relieve\n",
        "    iaa.Sometimes(0.2, iaa.EdgeDetect(alpha=(0.0, 0.7))), # Detecci√≥n de bordes\n",
        "    iaa.Sometimes(0.2, iaa.MotionBlur(k=(3, 7), angle=[-45, 45])), # Desenfoque de movimiento\n",
        "    iaa.Sometimes(0.2, iaa.AddToHueAndSaturation((-20, 20))) # Cambios de tono y saturaci√≥n\n",
        "], random_order=True) # Aplicar aumentaciones en orden aleatorio\n",
        "\n",
        "print(\"‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\")\n",
        "\n",
        "# ================= 3. & 4. CARGAR COCO ORIGINAL Y MAPPING =================\n",
        "# `coco_data` y `imagenes` ya est√°n disponibles desde la ejecuci√≥n anterior.\n",
        "# Asegur√©monos de tener un mapa de image_id a info de imagen y anotaciones\n",
        "\n",
        "original_images_info = {img['id']: img for img in coco_data.get('images', [])}\n",
        "original_annotations_info = {}\n",
        "for ann in coco_data.get('annotations', []):\n",
        "    image_id = ann['image_id']\n",
        "    if image_id not in original_annotations_info:\n",
        "        original_annotations_info[image_id] = []\n",
        "    original_annotations_info[image_id].append(ann)\n",
        "\n",
        "print(f\"‚úÖ COCO original cargado. Im√°genes: {len(original_images_info)}, Anotaciones para {len(original_annotations_info)} im√°genes.\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Directorios para el dataset aumentado creados en: /content/alopecia_dataset/augmented_dataset\n",
            "‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'coco_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1148321399.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Asegur√©monos de tener un mapa de image_id a info de imagen y anotaciones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0moriginal_images_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoco_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0moriginal_annotations_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoco_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'coco_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53156ae4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because `coco_data` and `imagenes` were not defined after the numpy downgrade. I need to re-load these variables from their respective paths (`json_path` and `images_dir`) to make them accessible for the data augmentation process. I'll add the necessary loading steps at the beginning of the cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "8116aca3",
        "outputId": "a4d15846-53f4-4bdd-fc5f-97972ae92b53"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "\n",
        "# ================= RELOAD NECESSARY DATA FROM PREVIOUS CELLS =================\n",
        "# These variables were defined in cell TPUKkd36GGgz and might have been lost\n",
        "# after runtime restart or numpy downgrade affecting the environment.\n",
        "\n",
        "# Re-define paths if not globally available, assuming previous cell execution state\n",
        "# (e.g., zip_path, output_dir, extract_dir, json_path, images_dir are defined from TPUKkd36GGgz)\n",
        "# If running this cell independently, ensure these paths are correctly set.\n",
        "\n",
        "# For demonstration, explicitly re-loading what's needed based on original notebook flow:\n",
        "# Assumed: `json_path` and `images_dir` from TPUKkd36GGgz are still valid string paths.\n",
        "\n",
        "# Load `coco_data`\n",
        "with open(json_path, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "print(f\"‚úÖ `coco_data` re-loaded from {json_path}.\")\n",
        "\n",
        "# Re-list `imagenes` (image paths found during extraction)\n",
        "imagenes = []\n",
        "for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.PNG']:\n",
        "    imagenes.extend(list(Path(images_dir).rglob(ext)))\n",
        "print(f\"‚úÖ `imagenes` re-listed: {len(imagenes)} found in {images_dir}.\")\n",
        "\n",
        "# ================= 1. DEFINIR DIRECTORIOS Y CONFIGURACI√ìN =================\n",
        "# Input: Directorio de extracci√≥n temporal del ZIP y JSON COCO original\n",
        "# `extract_dir` y `json_path` deben venir de la celda de extracci√≥n (TPUKkd36GGgz)\n",
        "# `coco_data` y `imagenes` tambi√©n de la celda TPUKkd36GGgz\n",
        "# `class_names` de la celda TPUKkd36GGgz\n",
        "\n",
        "# Output: Directorio para el dataset aumentado\n",
        "augmented_output_dir = Path('/content/alopecia_dataset/augmented_dataset')\n",
        "augmented_img_dir = augmented_output_dir / 'images'\n",
        "augmented_annot_dir = augmented_output_dir / 'annotations'\n",
        "\n",
        "# Limpiar y crear directorios\n",
        "shutil.rmtree(augmented_output_dir, ignore_errors=True)\n",
        "augmented_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "augmented_annot_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Directorios para el dataset aumentado creados en: {augmented_output_dir}\")\n",
        "\n",
        "# ================= 2. INICIALIZAR AUMENTADORES imgaug =================\n",
        "# Definir la secuencia de aumentaci√≥n. Usamos `random_order=True` para variar el orden.\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5), # Volteo horizontal con 50% de probabilidad\n",
        "    iaa.Affine(\n",
        "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # Escala 80-120%\n",
        "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # Traslaci√≥n -20% a +20%\n",
        "        rotate=(-25, 25), # Rotaci√≥n -25 a +25 grados\n",
        "        shear=(-8, 8) # Cizallamiento -8 a +8 grados\n",
        "    ),\n",
        "    iaa.Multiply((0.8, 1.2), per_channel=0.2), # Ajuste de brillo (80-120%)\n",
        "    iaa.LinearContrast((0.75, 1.5)), # Ajuste de contraste\n",
        "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # Ruido Gaussiano\n",
        "    iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 1.5))), # Afilado\n",
        "    iaa.Sometimes(0.2, iaa.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))), # Relieve\n",
        "    iaa.Sometimes(0.2, iaa.EdgeDetect(alpha=(0.0, 0.7))), # Detecci√≥n de bordes\n",
        "    iaa.Sometimes(0.2, iaa.MotionBlur(k=(3, 7), angle=[-45, 45])), # Desenfoque de movimiento\n",
        "    iaa.Sometimes(0.2, iaa.AddToHueAndSaturation((-20, 20))) # Cambios de tono y saturaci√≥n\n",
        "], random_order=True) # Aplicar aumentaciones en orden aleatorio\n",
        "\n",
        "print(\"‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\")\n",
        "\n",
        "# ================= 3. & 4. CARGAR COCO ORIGINAL Y MAPPING =================\n",
        "# `coco_data` y `imagenes` ya est√°n disponibles desde la ejecuci√≥n anterior.\n",
        "# Asegur√©monos de tener un mapa de image_id a info de imagen y anotaciones\n",
        "\n",
        "original_images_info = {img['id']: img for img in coco_data.get('images', [])}\n",
        "original_annotations_info = {}\n",
        "for ann in coco_data.get('annotations', []):\n",
        "    image_id = ann['image_id']\n",
        "    if image_id not in original_annotations_info:\n",
        "        original_annotations_info[image_id] = []\n",
        "    original_annotations_info[image_id].append(ann)\n",
        "\n",
        "print(f\"‚úÖ COCO original cargado. Im√°genes: {len(original_images_info)}, Anotaciones para {len(original_annotations_info)} im√°genes.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'json_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1728623412.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Load `coco_data`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mcoco_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ `coco_data` re-loaded from {json_path}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e73290a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because `json_path` and `images_dir` were undefined. I need to explicitly define these variables with their expected values, which can be inferred from the output of the initial data extraction cell (`TPUKkd36GGgz`). This will ensure `coco_data` and `imagenes` can be re-loaded successfully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b86d1936",
        "outputId": "75225d5b-f555-4731-b7f0-585cb079c59e"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "\n",
        "# ================= RELOAD NECESSARY DATA FROM PREVIOUS CELLS =================\n",
        "# These variables were defined in cell TPUKkd36GGgz and might have been lost\n",
        "# after runtime restart or numpy downgrade affecting the environment.\n",
        "\n",
        "# Explicitly define `json_path` and `images_dir` based on prior execution's output\n",
        "json_path = '/content/temp_extract/test-coco/result_coco.json'\n",
        "images_dir = '/content/temp_extract/test-coco/images'\n",
        "extract_dir = '/content/temp_extract'\n",
        "\n",
        "# Load `coco_data`\n",
        "with open(json_path, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "print(f\"‚úÖ `coco_data` re-loaded from {json_path}.\")\n",
        "\n",
        "# Re-list `imagenes` (image paths found during extraction)\n",
        "imagenes = []\n",
        "for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.PNG']:\n",
        "    imagenes.extend(list(Path(images_dir).rglob(ext)))\n",
        "print(f\"‚úÖ `imagenes` re-listed: {len(imagenes)} found in {images_dir}.\")\n",
        "\n",
        "# ================= 1. DEFINIR DIRECTORIOS Y CONFIGURACI√ìN =================\n",
        "# Output: Directorio para el dataset aumentado\n",
        "augmented_output_dir = Path('/content/alopecia_dataset/augmented_dataset')\n",
        "augmented_img_dir = augmented_output_dir / 'images'\n",
        "augmented_annot_dir = augmented_output_dir / 'annotations'\n",
        "\n",
        "# Limpiar y crear directorios\n",
        "shutil.rmtree(augmented_output_dir, ignore_errors=True)\n",
        "augmented_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "augmented_annot_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Directorios para el dataset aumentado creados en: {augmented_output_dir}\")\n",
        "\n",
        "# ================= 2. INICIALIZAR AUMENTADORES imgaug =================\n",
        "# Definir la secuencia de aumentaci√≥n. Usamos `random_order=True` para variar el orden.\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5), # Volteo horizontal con 50% de probabilidad\n",
        "    iaa.Affine(\n",
        "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # Escala 80-120%\n",
        "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # Traslaci√≥n -20% a +20%\n",
        "        rotate=(-25, 25), # Rotaci√≥n -25 a +25 grados\n",
        "        shear=(-8, 8) # Cizallamiento -8 a +8 grados\n",
        "    ),\n",
        "    iaa.Multiply((0.8, 1.2), per_channel=0.2), # Ajuste de brillo (80-120%)\n",
        "    iaa.LinearContrast((0.75, 1.5)), # Ajuste de contraste\n",
        "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # Ruido Gaussiano\n",
        "    iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 1.5))), # Afilado\n",
        "    iaa.Sometimes(0.2, iaa.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))), # Relieve\n",
        "    iaa.Sometimes(0.2, iaa.EdgeDetect(alpha=(0.0, 0.7))), # Detecci√≥n de bordes\n",
        "    iaa.Sometimes(0.2, iaa.MotionBlur(k=(3, 7), angle=[-45, 45])), # Desenfoque de movimiento\n",
        "    iaa.Sometimes(0.2, iaa.AddToHueAndSaturation((-20, 20))) # Cambios de tono y saturaci√≥n\n",
        "], random_order=True) # Aplicar aumentaciones en orden aleatorio\n",
        "\n",
        "print(\"‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\")\n",
        "\n",
        "# ================= 3. & 4. CARGAR COCO ORIGINAL Y MAPPING =================\n",
        "# `coco_data` y `imagenes` ya est√°n disponibles desde la ejecuci√≥n anterior.\n",
        "# Asegur√©monos de tener un mapa de image_id a info de imagen y anotaciones\n",
        "\n",
        "original_images_info = {img['id']: img for img in coco_data.get('images', [])}\n",
        "original_annotations_info = {}\n",
        "for ann in coco_data.get('annotations', []):\n",
        "    image_id = ann['image_id']\n",
        "    if image_id not in original_annotations_info:\n",
        "        original_annotations_info[image_id] = []\n",
        "    original_annotations_info[image_id].append(ann)\n",
        "\n",
        "print(f\"‚úÖ COCO original cargado. Im√°genes: {len(original_images_info)}, Anotaciones para {len(original_annotations_info)} im√°genes.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ `coco_data` re-loaded from /content/temp_extract/test-coco/result_coco.json.\n",
            "‚úÖ `imagenes` re-listed: 15 found in /content/temp_extract/test-coco/images.\n",
            "‚úÖ Directorios para el dataset aumentado creados en: /content/alopecia_dataset/augmented_dataset\n",
            "‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\n",
            "‚úÖ COCO original cargado. Im√°genes: 12, Anotaciones para 12 im√°genes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0564f6db"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to iterate through each original image, convert its COCO segmentation annotations into `imgaug` compatible format (SegmentationMapsOnImage), and prepare for augmentation. This involves finding the corresponding image file and processing its segmentation data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "3fb1c8c9",
        "outputId": "c391d208-6313-433d-8efb-2c7537d8be85"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points=points, label=ann['category_id']))\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "        polygons_aug = seq_det.augment_polygons(segmentations_ia)\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.points) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = poly_ia.points.flatten().tolist()\n",
        "                x_min, y_min, x_max, y_max = poly_ia.bounding_box.x1,\n",
        "                                             poly_ia.bounding_box.y1,\n",
        "                                             poly_ia.bounding_box.x2,\n",
        "                                             poly_ia.bounding_box.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-1662541983.py, line 125)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1662541983.py\"\u001b[0;36m, line \u001b[0;32m125\u001b[0m\n\u001b[0;31m    poly_ia.bounding_box.y1,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae707406"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to an `IndentationError` around line 125. This means there's an incorrect spacing in the code. I will fix the indentation to ensure the `bbox_aug` assignment is properly formatted.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e38180dd"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points=points, label=ann['category_id']))\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "        polygons_aug = seq_det.augment_polygons(segmentations_ia)\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.points) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = poly_ia.points.flatten().tolist()\n",
        "                x_min, y_min, x_max, y_max = poly_ia.bounding_box.x1,\n",
        "                                             poly_ia.bounding_box.y1,\n",
        "                                             poly_ia.bounding_box.x2,\n",
        "                                             poly_ia.bounding_box.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a5b7a19"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to an `IndentationError` in the multi-line assignment of bounding box coordinates. I will fix this by placing the assignment on a single line to ensure correct indentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0f097ed"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points=points, label=ann['category_id']))\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "        polygons_aug = seq_det.augment_polygons(segmentations_ia)\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.points) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = poly_ia.points.flatten().tolist()\n",
        "                # Fix: ensure bounding box assignment is on a single line\n",
        "                x_min, y_min, x_max, y_max = poly_ia.bounding_box.x1, poly_ia.bounding_box.y1, poly_ia.bounding_box.x2, poly_ia.bounding_box.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc1678b1"
      },
      "source": [
        "**Reasoning**:\n",
        "The `TypeError: Polygon.__init__() got an unexpected keyword argument 'points'` indicates that the `imgaug.Polygon` constructor expects the points as a positional argument, not a keyword argument. I will fix this by passing `points` directly as the first argument.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14cc6ae7"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id'])) # FIX: Remove 'points=' keyword\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "        polygons_aug = seq_det.augment_polygons(segmentations_ia)\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.points) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = poly_ia.points.flatten().tolist()\n",
        "                x_min, y_min, x_max, y_max = poly_ia.bounding_box.x1, poly_ia.bounding_box.y1, poly_ia.bounding_box.x2, poly_ia.bounding_box.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fb61a47"
      },
      "source": [
        "**Reasoning**:\n",
        "The `ValueError` indicates that `imgaug` needs the image shape to correctly augment polygons. I will provide the `image_original.shape` to the `augment_polygons` method to resolve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a740a49"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id'])) # FIX: Remove 'points=' keyword\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "        polygons_aug = seq_det.augment_polygons(segmentations_ia, shape=image_original.shape)\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.points) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = poly_ia.points.flatten().tolist()\n",
        "                x_min, y_min, x_max, y_max = poly_ia.bounding_box.x1, poly_ia.bounding_box.y1, poly_ia.bounding_box.x2, poly_ia.bounding_box.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d49d2ed"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `augment_polygons` received an unexpected `shape` argument. The `imgaug` library expects segmentation annotations to be encapsulated in `ia.PolygonsOnImage` objects along with the image shape when augmenting. I will wrap the `ia.Polygon` objects in an `ia.PolygonsOnImage` instance and then pass this to the `augment_polygons` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f08702a8"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id']))\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "\n",
        "        # FIX: Wrap segmentations_ia in PolygonsOnImage for augmentation\n",
        "        polygons_on_image = ia.PolygonsOnImage(segmentations_ia, shape=image_original.shape)\n",
        "        polygons_aug_on_image = seq_det.augment_polygons([polygons_on_image])[0] # Augment and unwrap from batch\n",
        "        polygons_aug = polygons_aug_on_image.polygons\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.points) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = poly_ia.points.flatten().tolist()\n",
        "                x_min, y_min, x_max, y_max = poly_ia.bounding_box.x1, poly_ia.bounding_box.y1, poly_ia.bounding_box.x2, poly_ia.bounding_box.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74a2d524"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `AttributeError: 'Polygon' object has no attribute 'points'` indicates that `poly_ia.points` is not the correct way to access the coordinates of an `imgaug.Polygon` object. I will fix this by accessing the x and y coordinates directly using `poly_ia.xx` and `poly_ia.yy` and then combining them into a NumPy array.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18986661"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id']))\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "\n",
        "        # FIX: Wrap segmentations_ia in PolygonsOnImage for augmentation\n",
        "        polygons_on_image = ia.PolygonsOnImage(segmentations_ia, shape=image_original.shape)\n",
        "        polygons_aug_on_image = seq_det.augment_polygons([polygons_on_image])[0] # Augment and unwrap from batch\n",
        "        polygons_aug = polygons_aug_on_image.polygons\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            # FIX: Access points using poly_ia.xx and poly_ia.yy instead of poly_ia.points\n",
        "            # Also adjust the check for minimum points accordingly\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.xx) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = np.column_stack([poly_ia.xx, poly_ia.yy]).flatten().tolist()\n",
        "                x_min, y_min, x_max, y_max = poly_ia.bounding_box.x1, poly_ia.bounding_box.y1, poly_ia.bounding_box.x2, poly_ia.bounding_box.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8afab869"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `imgaug.Polygon` objects do not have a `bounding_box` attribute directly. I need to call the `to_bounding_box()` method on the polygon object to obtain its bounding box and then extract the coordinates (`x1`, `y1`, `x2`, `y2`) from the resulting `BoundingBox` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd8e361b"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id']))\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "\n",
        "        # FIX: Wrap segmentations_ia in PolygonsOnImage for augmentation\n",
        "        polygons_on_image = ia.PolygonsOnImage(segmentations_ia, shape=image_original.shape)\n",
        "        polygons_aug_on_image = seq_det.augment_polygons([polygons_on_image])[0] # Augment and unwrap from batch\n",
        "        polygons_aug = polygons_aug_on_image.polygons\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            # FIX: Access points using poly_ia.xx and poly_ia.yy instead of poly_ia.points\n",
        "            # Also adjust the check for minimum points accordingly\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.xx) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = np.column_stack([poly_ia.xx, poly_ia.yy]).flatten().tolist()\n",
        "                # FIX: Get bounding box from the polygon object\n",
        "                bbox_obj = poly_ia.to_bounding_box()\n",
        "                x_min, y_min, x_max, y_max = bbox_obj.x1, bbox_obj.y1, bbox_obj.x2, bbox_obj.y2\n",
        "                bbox_aug = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": poly_ia.area,\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21a60ef9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `TypeError: Object of type float32 is not JSON serializable` when trying to save the `augmented_coco_data` to JSON. This means some numerical values (likely from bounding boxes or segmentations) are still in NumPy's `float32` format. I need to convert these `float32` values to standard Python floats before dumping the JSON data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6445e899"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "new_image_id = max([img['id'] for img in coco_data['images']]) + 1 if coco_data.get('images') else 0\n",
        "new_annotation_id = max([ann['id'] for ann in coco_data['annotations']]) + 1 if coco_data.get('annotations') else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            # imgaug expects polygons as a list of points [[x1, y1], [x2, y2], ...]\n",
        "            # COCO segmentation for polygons is [x1, y1, x2, y2, ...]\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    # Reshape [x1, y1, x2, y2, ...] to [[x1, y1], [x2, y2], ...]\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id']))\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas. Se copiar√° tal cual.\")\n",
        "        # Add original image and its annotations to the augmented dataset\n",
        "        augmented_coco_data[\"images\"].append(original_img_info)\n",
        "        for ann in original_annotations:\n",
        "            augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': original_img_info['id'],\n",
        "            'aligned_image': original_img_info['file_name'],\n",
        "            'annotation_count': len(original_annotations)\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    print(f\"  Procesando original ID {original_img_id} ('{original_file_name}') con {len(segmentations_ia)} segmentaciones.\")\n",
        "\n",
        "    # Add original image and its annotations to the augmented dataset (as the first 'augmented' version, X=0)\n",
        "    augmented_coco_data[\"images\"].append(original_img_info)\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_info['id'],\n",
        "        'aligned_image': original_img_info['file_name'],\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        # Make a copy of the sequence to ensure different random operations for each augmentation\n",
        "        seq_det = seq.to_deterministic()\n",
        "\n",
        "        # Augment image and polygons\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "\n",
        "        # FIX: Wrap segmentations_ia in PolygonsOnImage for augmentation\n",
        "        polygons_on_image = ia.PolygonsOnImage(segmentations_ia, shape=image_original.shape)\n",
        "        polygons_aug_on_image = seq_det.augment_polygons([polygons_on_image])[0] # Augment and unwrap from batch\n",
        "        polygons_aug = polygons_aug_on_image.polygons\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        new_image_filename = f\"{Path(original_file_name).stem}_aug{aug_idx}{Path(original_file_name).suffix}\"\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            # FIX: Access points using poly_ia.xx and poly_ia.yy instead of poly_ia.points\n",
        "            # Also adjust the check for minimum points accordingly\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.xx) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = np.column_stack([poly_ia.xx, poly_ia.yy]).flatten().tolist()\n",
        "                # Ensure all elements in coco_segmentation are standard Python floats\n",
        "                coco_segmentation = [float(p) for p in coco_segmentation]\n",
        "\n",
        "                # FIX: Get bounding box from the polygon object\n",
        "                bbox_obj = poly_ia.to_bounding_box()\n",
        "                x_min, y_min, x_max, y_max = bbox_obj.x1, bbox_obj.y1, bbox_obj.x2, bbox_obj.y2\n",
        "                bbox_aug = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": float(poly_ia.area),\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d7c9f45"
      },
      "source": [
        "## Update Training Script for Augmented Data\n",
        "\n",
        "### Subtask:\n",
        "Modify the `aligned_dir` variable in the existing training script (cell `KH6EzZzmVYGT`) to point to the newly created directory containing the augmented dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19a66a68"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# ----------------------------\n",
        "# 1. DATASET PARA ENTRENAMIENTO\n",
        "# ----------------------------\n",
        "class AlopeciaDataset(Dataset):\n",
        "    def __init__(self, aligned_dir, img_size=256):\n",
        "        self.img_dir = os.path.join(aligned_dir, 'images')\n",
        "        self.annot_dir = os.path.join(aligned_dir, 'annotations')\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Cargar informaci√≥n del dataset\n",
        "        info_path = os.path.join(aligned_dir, 'annotations', 'dataset_info.json')\n",
        "        with open(info_path, 'r') as f:\n",
        "            self.dataset_info = json.load(f)\n",
        "\n",
        "        # Tambi√©n cargar COCO filtrado\n",
        "        coco_path = os.path.join(aligned_dir, 'annotations', 'filtered_coco.json')\n",
        "        with open(coco_path, 'r') as f:\n",
        "            self.coco_data = json.load(f)\n",
        "\n",
        "        print(f\"Dataset cargado: {len(self.dataset_info)} im√°genes, {sum(item['annotation_count'] for item in self.dataset_info)} anotaciones\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset_info[idx]\n",
        "        img_path = os.path.join(self.img_dir, item['aligned_image'])\n",
        "\n",
        "        # Cargar imagen\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "        img = img / 255.0  # Normalizar [0, 1]\n",
        "        img = torch.tensor(img).permute(2, 0, 1).float()\n",
        "\n",
        "        # Crear m√°scara\n",
        "        mask = self._create_mask_from_coco(item['id'], self.img_size)\n",
        "        mask = torch.tensor(mask).unsqueeze(0).float()\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "    def _create_mask_from_coco(self, image_id, size):\n",
        "        mask = np.zeros((size, size), dtype=np.uint8)\n",
        "\n",
        "        # Get image info\n",
        "        image_info = None\n",
        "        for img_info in self.coco_data['images']:\n",
        "            if img_info['id'] == image_id:\n",
        "                image_info = img_info\n",
        "                break\n",
        "        if image_info is None:\n",
        "            raise ValueError(f\"Image info not found for image_id: {image_id}\")\n",
        "\n",
        "        img_width = image_info['width']\n",
        "        img_height = image_info['height']\n",
        "\n",
        "        # Buscar anotaciones\n",
        "        for ann in self.coco_data['annotations']:\n",
        "            if ann['image_id'] == image_id:\n",
        "                seg = ann['segmentation']\n",
        "                if isinstance(seg, list):\n",
        "                    for polygon in seg:\n",
        "                        pts = np.array(polygon).reshape(-1, 2)\n",
        "                        pts = (pts * size / max(img_width, img_height)).astype(np.int32)\n",
        "                        cv2.fillPoly(mask, [pts], 1)\n",
        "\n",
        "        return mask\n",
        "\n",
        "# ----------------------------\n",
        "# 2. MODELO MEJORADO (Sin Sigmoid al final)\n",
        "# ----------------------------\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.enc2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(192, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec2 = nn.Sequential(\n",
        "            nn.Conv2d(67, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Salida (logits, sin sigmoid)\n",
        "        self.output = nn.Conv2d(64, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        b = self.bottleneck(e2)\n",
        "\n",
        "        d1 = self.up1(b)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = torch.cat([d2, x[:, :, :d2.shape[2], :d2.shape[3]]], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        out = self.output(d2)\n",
        "        return out # <-- Dejamos los logits crudos\n",
        "\n",
        "# ----------------------------\n",
        "# 3. ENTRENAMIENTO (Con pesos para clases desbalanceadas)\n",
        "# ----------------------------\n",
        "def train_model():\n",
        "    aligned_dir = '/content/alopecia_dataset/augmented_dataset'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "    dataset = AlopeciaDataset(aligned_dir, img_size=256)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    model = SimpleUNet().to(device)\n",
        "\n",
        "    # PESO PARA LA CLASE POSITIVA (ALOPECIA)\n",
        "    # Esto penaliza 10 veces m√°s equivocarse en la alopecia que en el fondo\n",
        "    pos_weight = torch.tensor([10.0]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 30\n",
        "    train_losses = []\n",
        "\n",
        "    print(\"\\n=== COMIENZO DEL ENTRENAMIENTO MEJORADO ===\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 2 == 0:\n",
        "                 print(f\"  Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint_path = f'/content/alopecia_dataset/model_checkpoint_epoch_{epoch+1}.pth'\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"  Checkpoint guardado: {checkpoint_path}\")\n",
        "\n",
        "    final_model_path = '/content/alopecia_dataset/alopecia_segmentation_model.pth'\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"\\n‚úÖ Modelo final guardado: {final_model_path}\")\n",
        "\n",
        "    return model, train_losses\n",
        "\n",
        "# ----------------------------\n",
        "# 4. VISUALIZACI√ìN MEJORADA (Mapas de calor)\n",
        "# ----------------------------\n",
        "def visualize_predictions(model, dataset, num_samples=3):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Visualizamos 3 filas, 4 columnas\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        img, true_mask = dataset[i]\n",
        "        img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(img_tensor)\n",
        "            probs = torch.sigmoid(logits) # Convertimos logits a prob (0-1)\n",
        "            pred_mask = (probs > 0.3).float() # Umbral bajado a 0.3\n",
        "\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "        true_mask_np = true_mask[0].cpu().numpy()\n",
        "        probs_np = probs[0][0].cpu().numpy()\n",
        "        pred_mask_np = pred_mask[0][0].cpu().numpy()\n",
        "\n",
        "        # Imagen\n",
        "        axes[i, 0].imshow(img_np)\n",
        "        axes[i, 0].set_title(f\"Imagen {i+1}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Real\n",
        "        axes[i, 1].imshow(true_mask_np, cmap='gray')\n",
        "        axes[i, 1].set_title(\"Real\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Calor (Probabilidades)\n",
        "        im = axes[i, 2].imshow(probs_np, cmap='jet', vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(\"Confianza (Mapa Calor)\")\n",
        "        axes[i, 2].axis('off')\n",
        "        plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
        "\n",
        "        # Predicci√≥n binaria\n",
        "        axes[i, 3].imshow(pred_mask_np, cmap='gray')\n",
        "        axes[i, 3].set_title(\"Predicci√≥n (>0.3)\")\n",
        "        axes[i, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ INICIANDO ENTRENAMIENTO DEL MODELO DE SEGMENTACI√ìN\")\n",
        "    try:\n",
        "        trained_model, losses = train_model()\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('√âpoca')\n",
        "        plt.ylabel('P√©rdida')\n",
        "        plt.title('Curva de Aprendizaje')\n",
        "        plt.grid(True)\n",
        "\n",
        "        aligned_dir = '/content/alopecia_dataset/augmented_dataset'\n",
        "        dataset = AlopeciaDataset(aligned_dir, img_size=256)\n",
        "\n",
        "        print(\"\\\\nüéØ VISUALIZANDO PREDICCIONES DEL MODELO\")\n",
        "        visualize_predictions(trained_model, dataset, num_samples=min(3, len(dataset)))\n",
        "\n",
        "        print(\"\\\\n‚úÖ ¬°ENTRENAMIENTO COMPLETADO!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la ejecuci√≥n: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a627d4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `cv2.imread` returned `None` due to image files not being present in the `augmented_img_dir` for all entries in the dataset. This happened because original images were not always explicitly copied to the augmented directory, especially if they had no segmentation annotations. The fix involves ensuring that every original image that is processed (regardless of annotations) is copied to the `augmented_img_dir` with a consistent filename (based on its COCO ID) and that its corresponding entry in `augmented_coco_data` and `augmented_dataset_info_list` reflects this filename. The augmented images will also use a consistent naming convention based on the original COCO ID.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f93e8c42"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "from imgaug import augmenters as iaa\n",
        "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
        "\n",
        "# ================= RELOAD NECESSARY DATA FROM PREVIOUS CELLS =================\n",
        "# These variables were defined in cell TPUKkd36GGgz and might have been lost\n",
        "# after runtime restart or numpy downgrade affecting the environment.\n",
        "\n",
        "# Explicitly define `json_path` and `images_dir` based on prior execution's output\n",
        "json_path = '/content/temp_extract/test-coco/result_coco.json'\n",
        "images_dir = '/content/temp_extract/test-coco/images'\n",
        "extract_dir = '/content/temp_extract'\n",
        "\n",
        "# Load `coco_data`\n",
        "with open(json_path, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "print(f\"‚úÖ `coco_data` re-loaded from {json_path}.\")\n",
        "\n",
        "# Re-list `imagenes` (image paths found during extraction)\n",
        "imagenes = []\n",
        "for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.PNG']:\n",
        "    imagenes.extend(list(Path(images_dir).rglob(ext)))\n",
        "print(f\"‚úÖ `imagenes` re-listed: {len(imagenes)} found in {images_dir}.\")\n",
        "\n",
        "# ================= 1. DEFINIR DIRECTORIOS Y CONFIGURACI√ìN =================\n",
        "# Output: Directorio para el dataset aumentado\n",
        "augmented_output_dir = Path('/content/alopecia_dataset/augmented_dataset')\n",
        "augmented_img_dir = augmented_output_dir / 'images'\n",
        "augmented_annot_dir = augmented_output_dir / 'annotations'\n",
        "\n",
        "# Limpiar y crear directorios\n",
        "shutil.rmtree(augmented_output_dir, ignore_errors=True)\n",
        "augmented_img_dir.mkdir(parents=True, exist_ok=True)\n",
        "augmented_annot_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Directorios para el dataset aumentado creados en: {augmented_output_dir}\")\n",
        "\n",
        "# ================= 2. INICIALIZAR AUMENTADORES imgaug =================\n",
        "# Definir la secuencia de aumentaci√≥n. Usamos `random_order=True` para variar el orden.\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5), # Volteo horizontal con 50% de probabilidad\n",
        "    iaa.Affine(\n",
        "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # Escala 80-120%\n",
        "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # Traslaci√≥n -20% a +20%\n",
        "        rotate=(-25, 25), # Rotaci√≥n -25 a +25 grados\n",
        "        shear=(-8, 8) # Cizallamiento -8 a +8 grados\n",
        "    ),\n",
        "    iaa.Multiply((0.8, 1.2), per_channel=0.2), # Ajuste de brillo (80-120%)\n",
        "    iaa.LinearContrast((0.75, 1.5)), # Ajuste de contraste\n",
        "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # Ruido Gaussiano\n",
        "    iaa.Sometimes(0.2, iaa.Sharpen(alpha=(0.0, 1.0), lightness=(0.75, 1.5))), # Afilado\n",
        "    iaa.Sometimes(0.2, iaa.Emboss(alpha=(0.0, 1.0), strength=(0.5, 1.5))), # Relieve\n",
        "    iaa.Sometimes(0.2, iaa.EdgeDetect(alpha=(0.0, 0.7))), # Detecci√≥n de bordes\n",
        "    iaa.Sometimes(0.2, iaa.MotionBlur(k=(3, 7), angle=[-45, 45])), # Desenfoque de movimiento\n",
        "    iaa.Sometimes(0.2, iaa.AddToHueAndSaturation((-20, 20))) # Cambios de tono y saturaci√≥n\n",
        "], random_order=True) # Aplicar aumentaciones en orden aleatorio\n",
        "\n",
        "print(\"‚úÖ Secuencia de aumentaci√≥n de imgaug inicializada.\")\n",
        "\n",
        "# ================= 3. & 4. CARGAR COCO ORIGINAL Y MAPPING =================\n",
        "# `coco_data` y `imagenes` ya est√°n disponibles desde la ejecuci√≥n anterior.\n",
        "# Asegur√©monos de tener un mapa de image_id a info de imagen y anotaciones\n",
        "\n",
        "original_images_info = {img['id']: img for img in coco_data.get('images', [])}\n",
        "original_annotations_info = {}\n",
        "for ann in coco_data.get('annotations', []):\n",
        "    image_id = ann['image_id']\n",
        "    if image_id not in original_annotations_info:\n",
        "        original_annotations_info[image_id] = []\n",
        "    original_annotations_info[image_id].append(ann)\n",
        "\n",
        "print(f\"‚úÖ COCO original cargado. Im√°genes: {len(original_images_info)}, Anotaciones para {len(original_annotations_info)} im√°genes.\")\n",
        "\n",
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "# Starting new_image_id and new_annotation_id from highest existing ID + 1\n",
        "existing_image_ids = [img['id'] for img in coco_data.get('images', [])]\n",
        "new_image_id = max(existing_image_ids) + 1 if existing_image_ids else 0\n",
        "\n",
        "existing_annotation_ids = [ann['id'] for ann in coco_data.get('annotations', [])]\n",
        "new_annotation_id = max(existing_annotation_ids) + 1 if existing_annotation_ids else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada en `imagenes`. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB) # Convert to RGB for imgaug\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id']))\n",
        "\n",
        "    # --- Handle the original image itself (always copy if successfully loaded) ---\n",
        "    # Use original COCO image ID as the basis for the filename\n",
        "    original_image_saved_name = f\"{original_img_id}{original_img_path.suffix}\"\n",
        "    cv2.imwrite(str(augmented_img_dir / original_image_saved_name), cv2.cvtColor(image_original, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Create a new image info dictionary for the COCO format, ensuring file_name points to the copied file\n",
        "    original_img_coco_entry = original_img_info.copy() # Make a copy to avoid modifying original_images_info\n",
        "    original_img_coco_entry['file_name'] = original_image_saved_name # Update filename to match the saved one\n",
        "    augmented_coco_data[\"images\"].append(original_img_coco_entry)\n",
        "\n",
        "    # Add original annotations\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_id, # Use original ID for this entry\n",
        "        'aligned_image': original_image_saved_name, # This is the key for AlopeciaDataset\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "    print(f\"  A√±adida imagen original ID {original_img_id} ('{original_file_name}') a dataset aumentado.\")\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas para aumentaci√≥n. Solo se a√±adi√≥ la versi√≥n original.\")\n",
        "        continue # Skip augmentation for this image if no valid segmentations\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        seq_det = seq.to_deterministic()\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "\n",
        "        # Wrap segmentations_ia in PolygonsOnImage for augmentation\n",
        "        polygons_on_image = ia.PolygonsOnImage(segmentations_ia, shape=image_original.shape)\n",
        "        polygons_aug_on_image = seq_det.augment_polygons([polygons_on_image])[0] # Augment and unwrap from batch\n",
        "        polygons_aug = polygons_aug_on_image.polygons\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        # Ensure augmented IDs are distinct from original IDs and from each other\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1 # Increment for the next augmented image\n",
        "        new_image_filename = f\"{original_img_id}_aug{aug_idx}{original_img_path.suffix}\" # Use original COCO ID as base\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry for augmented image\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.xx) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = np.column_stack([poly_ia.xx, poly_ia.yy]).flatten().tolist()\n",
        "                # Ensure all elements in coco_segmentation are standard Python floats\n",
        "                coco_segmentation = [float(p) for p in coco_segmentation]\n",
        "\n",
        "                # Get bounding box from the polygon object\n",
        "                bbox_obj = poly_ia.to_bounding_box()\n",
        "                x_min, y_min, x_max, y_max = bbox_obj.x1, bbox_obj.y1, bbox_obj.x2, bbox_obj.y2\n",
        "                bbox_aug = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": float(poly_ia.area), # Ensure JSON serializable float\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1 # Increment for the next annotation\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59eea2bd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous errors in data augmentation were resolved, and the augmented dataset generation completed successfully. Now, I will re-execute the training script in cell `KH6EzZzmVYGT` with `aligned_dir` pointing to the newly generated augmented dataset to ensure it runs without `cv2.resize` assertion errors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "010020a1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# ----------------------------\n",
        "# 1. DATASET PARA ENTRENAMIENTO\n",
        "# ----------------------------\n",
        "class AlopeciaDataset(Dataset):\n",
        "    def __init__(self, aligned_dir, img_size=256):\n",
        "        self.img_dir = os.path.join(aligned_dir, 'images')\n",
        "        self.annot_dir = os.path.join(aligned_dir, 'annotations')\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Cargar informaci√≥n del dataset\n",
        "        info_path = os.path.join(aligned_dir, 'annotations', 'dataset_info.json')\n",
        "        with open(info_path, 'r') as f:\n",
        "            self.dataset_info = json.load(f)\n",
        "\n",
        "        # Tambi√©n cargar COCO filtrado\n",
        "        coco_path = os.path.join(aligned_dir, 'annotations', 'filtered_coco.json')\n",
        "        with open(coco_path, 'r') as f:\n",
        "            self.coco_data = json.load(f)\n",
        "\n",
        "        print(f\"Dataset cargado: {len(self.dataset_info)} im√°genes, {sum(item['annotation_count'] for item in self.dataset_info)} anotaciones\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset_info[idx]\n",
        "        img_path = os.path.join(self.img_dir, item['aligned_image'])\n",
        "\n",
        "        # Cargar imagen\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "        img = img / 255.0  # Normalizar [0, 1]\n",
        "        img = torch.tensor(img).permute(2, 0, 1).float()\n",
        "\n",
        "        # Crear m√°scara\n",
        "        mask = self._create_mask_from_coco(item['id'], self.img_size)\n",
        "        mask = torch.tensor(mask).unsqueeze(0).float()\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "    def _create_mask_from_coco(self, image_id, size):\n",
        "        mask = np.zeros((size, size), dtype=np.uint8)\n",
        "\n",
        "        # Get image info\n",
        "        image_info = None\n",
        "        for img_info in self.coco_data['images']:\n",
        "            if img_info['id'] == image_id:\n",
        "                image_info = img_info\n",
        "                break\n",
        "        if image_info is None:\n",
        "            raise ValueError(f\"Image info not found for image_id: {image_id}\")\n",
        "\n",
        "        img_width = image_info['width']\n",
        "        img_height = image_info['height']\n",
        "\n",
        "        # Buscar anotaciones\n",
        "        for ann in self.coco_data['annotations']:\n",
        "            if ann['image_id'] == image_id:\n",
        "                seg = ann['segmentation']\n",
        "                if isinstance(seg, list):\n",
        "                    for polygon in seg:\n",
        "                        pts = np.array(polygon).reshape(-1, 2)\n",
        "                        pts = (pts * size / max(img_width, img_height)).astype(np.int32)\n",
        "                        cv2.fillPoly(mask, [pts], 1)\n",
        "\n",
        "        return mask\n",
        "\n",
        "# ----------------------------\n",
        "# 2. MODELO MEJORADO (Sin Sigmoid al final)\n",
        "# ----------------------------\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.enc2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(192, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec2 = nn.Sequential(\n",
        "            nn.Conv2d(67, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Salida (logits, sin sigmoid)\n",
        "        self.output = nn.Conv2d(64, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        b = self.bottleneck(e2)\n",
        "\n",
        "        d1 = self.up1(b)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = torch.cat([d2, x[:, :, :d2.shape[2], :d2.shape[3]]], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        out = self.output(d2)\n",
        "        return out # <-- Dejamos los logits crudos\n",
        "\n",
        "# ----------------------------\n",
        "# 3. ENTRENAMIENTO (Con pesos para clases desbalanceadas)\n",
        "# ----------------------------\n",
        "def train_model():\n",
        "    aligned_dir = '/content/alopecia_dataset/augmented_dataset'\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "    dataset = AlopeciaDataset(aligned_dir, img_size=256)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    model = SimpleUNet().to(device)\n",
        "\n",
        "    # PESO PARA LA CLASE POSITIVA (ALOPECIA)\n",
        "    # Esto penaliza 10 veces m√°s equivocarse en la alopecia que en el fondo\n",
        "    pos_weight = torch.tensor([10.0]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 30\n",
        "    train_losses = []\n",
        "\n",
        "    print(\"\\n=== COMIENZO DEL ENTRENAMIENTO MEJORADO ===\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 2 == 0:\n",
        "                 print(f\"  Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint_path = f'/content/alopecia_dataset/model_checkpoint_epoch_{epoch+1}.pth'\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"  Checkpoint guardado: {checkpoint_path}\")\n",
        "\n",
        "    final_model_path = '/content/alopecia_dataset/alopecia_segmentation_model.pth'\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"\\n‚úÖ Modelo final guardado: {final_model_path}\")\n",
        "\n",
        "    return model, train_losses\n",
        "\n",
        "# ----------------------------\n",
        "# 4. VISUALIZACI√ìN MEJORADA (Mapas de calor)\n",
        "# ----------------------------\n",
        "def visualize_predictions(model, dataset, num_samples=3):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Visualizamos 3 filas, 4 columnas\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        img, true_mask = dataset[i]\n",
        "        img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(img_tensor)\n",
        "            probs = torch.sigmoid(logits) # Convertimos logits a prob (0-1)\n",
        "            pred_mask = (probs > 0.3).float() # Umbral bajado a 0.3\n",
        "\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "        true_mask_np = true_mask[0].cpu().numpy()\n",
        "        probs_np = probs[0][0].cpu().numpy()\n",
        "        pred_mask_np = pred_mask[0][0].cpu().numpy()\n",
        "\n",
        "        # Imagen\n",
        "        axes[i, 0].imshow(img_np)\n",
        "        axes[i, 0].set_title(f\"Imagen {i+1}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Real\n",
        "        axes[i, 1].imshow(true_mask_np, cmap='gray')\n",
        "        axes[i, 1].set_title(\"Real\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Calor (Probabilidades)\n",
        "        im = axes[i, 2].imshow(probs_np, cmap='jet', vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(\"Confianza (Mapa Calor)\")\n",
        "        axes[i, 2].axis('off')\n",
        "        plt.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
        "\n",
        "        # Predicci√≥n binaria\n",
        "        axes[i, 3].imshow(pred_mask_np, cmap='gray')\n",
        "        axes[i, 3].set_title(\"Predicci√≥n (>0.3)\")\n",
        "        axes[i, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚ú® INICIANDO ENTRENAMIENTO DEL MODELO DE SEGMENTACI√ìN\")\n",
        "    try:\n",
        "        trained_model, losses = train_model()\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('√âpoca')\n",
        "        plt.ylabel('P√©rdida')\n",
        "        plt.title('Curva de Aprendizaje')\n",
        "        plt.grid(True)\n",
        "\n",
        "        aligned_dir = '/content/alopecia_dataset/augmented_dataset'\n",
        "        dataset = AlopeciaDataset(aligned_dir, img_size=256)\n",
        "\n",
        "        print(\"\\\\nüéØ VISUALIZANDO PREDICCIONES DEL MODELO\")\n",
        "        visualize_predictions(trained_model, dataset, num_samples=min(3, len(dataset)))\n",
        "\n",
        "        print(\"\\\\n‚úÖ ¬°ENTRENAMIENTO COMPLETADO!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la ejecuci√≥n: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "082b3586"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire process, confirming the successful generation of synthetic data, the training of the segmentation model with this augmented dataset, and any observed improvements or next steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5864178"
      },
      "source": [
        "# Task\n",
        "**Summary of the Refactoring Process and Model Training:**\n",
        "\n",
        "The entire process of setting up, preparing data, augmenting, training, and testing a segmentation model for alopecia detection has been successfully refactored and executed.\n",
        "\n",
        "Here's a breakdown of the key steps and their successful completion:\n",
        "\n",
        "1.  **Consolidated Initial Setup and Data Extraction**:\n",
        "    *   Google Drive was successfully mounted.\n",
        "    *   The COCO dataset ZIP file (`test-coco-upload.zip`) was extracted to a temporary directory (`/content/temp_extract`).\n",
        "    *   COCO JSON annotations (`/content/temp_extract/test-coco/result_coco.json`) and the list of image paths were loaded, initializing `coco_data`, `imagenes`, `json_path`, `images_dir`, and `extract_dir` for subsequent steps.\n",
        "\n",
        "2.  **Consolidated Library Installations and NumPy Downgrade**:\n",
        "    *   Necessary Python libraries (`imgaug`, `opencv-python`, `scikit-image`) were installed.\n",
        "    *   A critical `numpy` downgrade to `1.26.4` was successfully performed to ensure compatibility with `imgaug`, resolving `AttributeError` related to `np.sctypes`.\n",
        "\n",
        "3.  **Refactored and Generated Augmented Segmentation Dataset**:\n",
        "    *   The code for generating synthetic images and their COCO annotations was refactored.\n",
        "    *   The process generated **110 images** and **180 annotations** in the `augmented_dataset` directory (`/content/alopecia_dataset/augmented_dataset`).\n",
        "    *   This involved applying various geometric and photometric transformations using `imgaug` to the original dataset.\n",
        "    *   Several issues encountered during augmentation were successfully resolved:\n",
        "        *   `NameError` for `coco_data` and `imagenes` was fixed by explicitly reloading these variables.\n",
        "        *   `IndentationError` in bounding box assignment was corrected.\n",
        "        *   `TypeError` when creating `ia.Polygon` was resolved by passing points as a positional argument.\n",
        "        *   `ValueError` related to missing image shape for `augment_polygons` was fixed by wrapping `ia.Polygon` objects in `ia.PolygonsOnImage` with the correct image shape.\n",
        "        *   `AttributeError` for `poly_ia.points` and `poly_ia.bounding_box` was resolved by using `poly_ia.xx`, `poly_ia.yy`, and `poly_ia.to_bounding_box()`.\n",
        "        *   `TypeError: Object of type float32 is not JSON serializable` was fixed by explicitly converting all numerical values (segmentation coordinates, bbox coordinates, area) to standard Python floats before JSON serialization.\n",
        "        *   `cv2.resize` assertion error was fixed by ensuring that all original images are copied to the `augmented_dataset` directory with consistent naming (using their COCO IDs) before augmentation, guaranteeing that all dataset entries point to existing image files.\n",
        "    *   The augmented images and their corresponding updated COCO JSON files (`filtered_coco.json`, `dataset_info.json`) were correctly saved in the `augmented_dataset` directory.\n",
        "\n",
        "4.  **Refactored and Trained Segmentation Model with Augmented Data**:\n",
        "    *   The `AlopeciaDataset` class, `SimpleUNet` model definition, and the training loop were consolidated and refactored.\n",
        "    *   The `SimpleUNet` model was successfully trained for **30 epochs** using the newly generated augmented dataset (`/content/alopecia_dataset/augmented_dataset`).\n",
        "    *   Checkpoints were saved every 5 epochs, and the final trained model (`/content/alopecia_dataset/alopecia_segmentation_model.pth`) was stored.\n",
        "    *   The training process now correctly loads images and masks from the augmented dataset, and the loss curve indicates the model's learning progression.\n",
        "\n",
        "5.  **Refactored and Visualized Model Predictions**:\n",
        "    *   The code for visualizing model predictions was refactored and successfully executed.\n",
        "    *   It now correctly loads the trained segmentation model, preprocesses a new image (from the extracted original dataset), performs prediction, and visualizes the original image, probability map, and binary mask.\n",
        "\n",
        "**Conclusion**:\n",
        "All components of the project, from initial data setup and augmentation to model training and testing, have been successfully executed and refactored. The generation of augmented data significantly increased the dataset size from 12 original images to 110 images, providing a more robust dataset for training the segmentation model. The model trained effectively on this expanded dataset, and its predictions can now be visualized. The next steps would involve further evaluation, potential hyperparameter tuning, and deployment considerations based on the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e140b6c5"
      },
      "source": [
        "## Consolidate Initial Setup and Data Extraction\n",
        "\n",
        "### Subtask:\n",
        "Merge and refactor all initial setup steps, including mounting Google Drive, extracting the COCO dataset ZIP file to a temporary directory, and loading the COCO JSON annotations along with the list of found image paths. This step will ensure that `coco_data`, `imagenes`, `json_path`, `images_dir`, and `extract_dir` are correctly initialized and accessible for subsequent steps in a clean manner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e6f2463"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to combine all initial setup and data extraction steps into a single code block as requested. This includes importing libraries, mounting Drive, defining paths, extracting the ZIP, finding JSON and image directories, listing image files, and loading the COCO JSON data. This ensures all required variables are correctly initialized and accessible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "212a5494"
      },
      "source": [
        "from google.colab import drive\n",
        "import os, json, shutil, zipfile\n",
        "from pathlib import Path\n",
        "import glob # Import glob for searching zip files\n",
        "\n",
        "# ================= 1. MONTAR GOOGLE DRIVE =================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ================= 2. CONFIGURACI√ìN =================\n",
        "# Ruta al ZIP en Google Drive (¬°AJUSTA ESTA RUTA SI ES NECESARIO!)\n",
        "zip_path = '/content/drive/MyDrive/alopecia_project/data/test-coco-upload.zip'\n",
        "\n",
        "# Ruta donde quieres guardar el dataset YOLO en Drive (se usar√° m√°s adelante)\n",
        "output_dir = '/content/drive/MyDrive/alopecia_project/yolo_dataset'\n",
        "\n",
        "# ================= 3. EXTRAER ZIP =================\n",
        "print(\"üì¶ Extrayendo ZIP de Google Drive...\")\n",
        "extract_dir = '/content/temp_extract'\n",
        "shutil.rmtree(extract_dir, ignore_errors=True)\n",
        "os.makedirs(extract_dir)\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        print(\"Contenido del ZIP:\")\n",
        "        file_list = zip_ref.namelist()\n",
        "        for file in file_list[:5]: # Mostrar primeros 5\n",
        "            print(f\"  - {file}\")\n",
        "        if len(file_list) > 5:\n",
        "            print(f\"  ... y {len(file_list) - 5} m√°s\")\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"‚úÖ ZIP extra√≠do en: {extract_dir}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå No se encontr√≥ el ZIP en: {zip_path}\")\n",
        "    print(\"\\nüìÅ Buscando archivos ZIP en Google Drive...\")\n",
        "    zip_files = glob.glob('/content/drive/MyDrive/**/*.zip', recursive=True)\n",
        "    if zip_files:\n",
        "        print(\"Archivos ZIP encontrados:\")\n",
        "        for zf in zip_files[:5]:\n",
        "            print(f\"  - {zf}\")\n",
        "        # Usar el primer ZIP encontrado como predeterminado\n",
        "        zip_path = zip_files[0]\n",
        "        print(f\"\\n‚úÖ Usando: {zip_path}\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f\"‚úÖ ZIP extra√≠do en: {extract_dir}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No se encontr√≥ ning√∫n archivo ZIP en Google Drive.\")\n",
        "\n",
        "# ================= 4. BUSCAR ARCHIVOS JSON Y CARPETA DE IM√ÅGENES =================\n",
        "# Buscar JSON\n",
        "json_files = list(Path(extract_dir).rglob('*.json'))\n",
        "if not json_files:\n",
        "    raise FileNotFoundError(\"‚ùå No se encontr√≥ ning√∫n archivo JSON en el ZIP extra√≠do.\")\n",
        "json_path = str(json_files[0])\n",
        "print(f\"‚úÖ JSON encontrado: {json_path}\")\n",
        "\n",
        "# Buscar carpeta de im√°genes\n",
        "images_dir = None\n",
        "possible_img_subdirs = ['images', 'img', 'upload', 'data', 'media']\n",
        "# First, check common subdirectories within the folder containing the JSON\n",
        "json_parent_dir = Path(json_path).parent\n",
        "for sub_dir_name in possible_img_subdirs:\n",
        "    candidate_images_dir = json_parent_dir / sub_dir_name\n",
        "    if candidate_images_dir.is_dir() and any(f.suffix.lower() in ['.jpg', '.png', '.jpeg', '.webp'] for f in candidate_images_dir.iterdir()):\n",
        "        images_dir = candidate_images_dir\n",
        "        break\n",
        "\n",
        "# If not found in common subdirectories, search more broadly within extract_dir\n",
        "if not images_dir:\n",
        "    for root, dirs, files in os.walk(extract_dir):\n",
        "        if any(f.lower().endswith(('.jpg', '.png', '.jpeg', '.webp')) for f in files):\n",
        "            images_dir = Path(root)\n",
        "            break\n",
        "\n",
        "if not images_dir:\n",
        "    print(\"‚ö†Ô∏è No se encontr√≥ una carpeta de im√°genes espec√≠fica. Se usar√° el directorio de extracci√≥n completo para la b√∫squeda.\")\n",
        "    images_dir = Path(extract_dir)\n",
        "\n",
        "print(f\"‚úÖ Carpeta de im√°genes identificada: {images_dir}\")\n",
        "\n",
        "# ================= 5. LISTAR IM√ÅGENES =================\n",
        "imagenes = []\n",
        "for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.PNG']:\n",
        "    imagenes.extend(list(images_dir.rglob(ext)))\n",
        "\n",
        "if not imagenes:\n",
        "    raise FileNotFoundError(f\"‚ùå No se encontraron im√°genes en: {images_dir}\")\n",
        "\n",
        "print(f\"üì∏ Total im√°genes encontradas para procesamiento: {len(imagenes)}\")\n",
        "for img_path in imagenes[:5]:\n",
        "    print(f\"  - {img_path.name}\")\n",
        "\n",
        "# ================= 6. CARGAR JSON COCO =================\n",
        "with open(json_path, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "print(f\"üìä Total de im√°genes en JSON (COCO): {len(coco_data.get('images', []))}\")\n",
        "print(f\"üìä Total de anotaciones en JSON (COCO): {len(coco_data.get('annotations', []))}\")\n",
        "print(\"‚úÖ `coco_data`, `imagenes`, `json_path`, `images_dir`, y `extract_dir` inicializados.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59692920"
      },
      "source": [
        "## Consolidate Library Installations and NumPy Downgrade\n",
        "\n",
        "### Subtask:\n",
        "Gather all necessary Python library installations (`imgaug`, `opencv-python`, `scikit-image`) and the `numpy` downgrade into a single, clean cell. This centralizes dependency management and ensures `imgaug` compatibility from the start.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24741f4e"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to consolidate all library installations and the `numpy` downgrade into a single code block. This will first uninstall any existing numpy, then install `numpy==1.26.4` for `imgaug` compatibility, and finally install `imgaug`, `opencv-python`, and `scikit-image`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7830e58"
      },
      "source": [
        "print('üîÑ Downgrading numpy for imgaug compatibility and installing other libraries...')\n",
        "# Uninstall existing numpy\n",
        "!pip uninstall -y numpy\n",
        "# Install compatible numpy version\n",
        "!pip install numpy==1.26.4\n",
        "# Install other necessary libraries\n",
        "!pip install imgaug opencv-python scikit-image\n",
        "print('‚úÖ Libraries installed and numpy downgraded.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96e4a156"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to downgrade numpy failed because `imgaug` re-installed a newer version. To ensure `numpy==1.26.4` remains installed, I will first install `imgaug` and other libraries to satisfy their dependencies, and then explicitly reinstall `numpy==1.26.4` using `--no-deps` to prevent its version from being overwritten.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e53db046"
      },
      "source": [
        "print('üîÑ Ensuring imgaug compatibility and installing other libraries...')\n",
        "\n",
        "# Uninstall any numpy version first to ensure a clean state\n",
        "!pip uninstall -y numpy\n",
        "\n",
        "# Install imgaug and other libraries, letting them install their preferred numpy (if any)\n",
        "!pip install imgaug opencv-python scikit-image\n",
        "\n",
        "# Force install numpy==1.26.4, without re-evaluating its dependencies\n",
        "# This ensures imgaug's numpy dependency (>=1.15) is met, but specifically locks to 1.26.4\n",
        "!pip install numpy==1.26.4 --no-deps\n",
        "\n",
        "print('‚úÖ Libraries installed and numpy downgraded to 1.26.4 successfully.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e721ff0e"
      },
      "source": [
        "## Refactor and Generate Augmented Segmentation Dataset\n",
        "\n",
        "### Subtask:\n",
        "Refactor the code responsible for generating synthetic images and their COCO annotations. This step will take the initial dataset and apply a series of geometric and photometric transformations using `imgaug` to create an augmented dataset (approximately 100 images). The code will be cleaned for readability and efficiency, ensuring that images and their corresponding updated COCO JSON files (`filtered_coco.json`, `dataset_info.json`) are correctly saved in a new `augmented_dataset` directory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ece07ef3"
      },
      "source": [
        "**Reasoning**:\n",
        "The user explicitly asked to execute the code in cell `bd8e361b` to generate the augmented dataset. This code leverages the `coco_data`, `imagenes`, `json_path`, `images_dir`, and `extract_dir` variables that were successfully initialized in the consolidated setup cell `212a5494`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28746416"
      },
      "source": [
        "ia.seed(1)\n",
        "\n",
        "augmented_coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": coco_data.get(\"categories\", []),\n",
        "    \"info\": coco_data.get(\"info\", {})\n",
        "}\n",
        "augmented_dataset_info_list = []\n",
        "\n",
        "# Keep track of new IDs for images and annotations\n",
        "existing_image_ids = [img['id'] for img in coco_data.get('images', [])]\n",
        "new_image_id = max(existing_image_ids) + 1 if existing_image_ids else 0\n",
        "\n",
        "existing_annotation_ids = [ann['id'] for ann in coco_data.get('annotations', [])]\n",
        "new_annotation_id = max(existing_annotation_ids) + 1 if existing_annotation_ids else 0\n",
        "\n",
        "num_augmentations_per_image = 9 # Generate 9 augmented versions for each original image\n",
        "\n",
        "print(f\"\\n--- Generando dataset aumentado (aproximadamente {len(original_images_info) * (1 + num_augmentations_per_image)} im√°genes) ---\")\n",
        "\n",
        "# Iterar sobre cada imagen original\n",
        "for original_img_id, original_img_info in original_images_info.items():\n",
        "    original_file_name = original_img_info.get('file_name')\n",
        "    original_width = original_img_info.get('width')\n",
        "    original_height = original_img_info.get('height')\n",
        "\n",
        "    if not original_file_name or original_width is None or original_height is None:\n",
        "        print(f\"‚ö†Ô∏è Informaci√≥n incompleta para imagen original ID {original_img_id}. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Find the actual path of the original image from the 'imagenes' list\n",
        "    original_img_path = None\n",
        "    nombre_limpio = os.path.basename(str(original_file_name))\n",
        "    if '?' in nombre_limpio:\n",
        "        nombre_limpio = nombre_limpio.split('?')[0]\n",
        "\n",
        "    for img_path_candidate in imagenes:\n",
        "        if img_path_candidate.name == nombre_limpio:\n",
        "            original_img_path = img_path_candidate\n",
        "            break\n",
        "\n",
        "    if original_img_path is None:\n",
        "        print(f\"‚ùå Imagen original '{nombre_limpio}' (ID: {original_img_id}) no encontrada en `imagenes`. Saltando.\")\n",
        "        continue\n",
        "\n",
        "    # Load original image\n",
        "    image_original = cv2.imread(str(original_img_path))\n",
        "    if image_original is None:\n",
        "        print(f\"‚ùå No se pudo cargar la imagen: {original_img_path}. Saltando.\")\n",
        "        continue\n",
        "    image_original = cv2.cvtColor(image_original, cv2.COLOR_BGR2RGB) # Convert to RGB for imgaug\n",
        "\n",
        "    # Process original annotations for imgaug\n",
        "    segmentations_ia = []\n",
        "    original_annotations = original_annotations_info.get(original_img_id, [])\n",
        "    for ann in original_annotations:\n",
        "        if 'segmentation' in ann and ann['segmentation']:\n",
        "            if isinstance(ann['segmentation'], list) and len(ann['segmentation']) > 0:\n",
        "                for poly_coords in ann['segmentation']:\n",
        "                    points = np.array(poly_coords).reshape(-1, 2)\n",
        "                    segmentations_ia.append(ia.Polygon(points, label=ann['category_id']))\n",
        "\n",
        "    # --- Handle the original image itself (always copy if successfully loaded) ---\n",
        "    # Use original COCO image ID as the basis for the filename\n",
        "    original_image_saved_name = f\"{original_img_id}{original_img_path.suffix}\"\n",
        "    cv2.imwrite(str(augmented_img_dir / original_image_saved_name), cv2.cvtColor(image_original, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Create a new image info dictionary for the COCO format, ensuring file_name points to the copied file\n",
        "    original_img_coco_entry = original_img_info.copy() # Make a copy to avoid modifying original_images_info\n",
        "    original_img_coco_entry['file_name'] = original_image_saved_name # Update filename to match the saved one\n",
        "    augmented_coco_data[\"images\"].append(original_img_coco_entry)\n",
        "\n",
        "    # Add original annotations\n",
        "    for ann in original_annotations:\n",
        "        augmented_coco_data[\"annotations\"].append(ann)\n",
        "\n",
        "    augmented_dataset_info_list.append({\n",
        "        'id': original_img_id, # Use original ID for this entry\n",
        "        'aligned_image': original_image_saved_name, # This is the key for AlopeciaDataset\n",
        "        'annotation_count': len(original_annotations)\n",
        "    })\n",
        "    print(f\"  A√±adida imagen original ID {original_img_id} ('{original_file_name}') a dataset aumentado.\")\n",
        "\n",
        "    # Only augment if there are segmentation annotations for the image\n",
        "    if not segmentations_ia:\n",
        "        print(f\"‚ö†Ô∏è Imagen ID {original_img_id} ('{original_file_name}') no tiene anotaciones de segmentaci√≥n v√°lidas para aumentaci√≥n. Solo se a√±adi√≥ la versi√≥n original.\")\n",
        "        continue # Skip augmentation for this image if no valid segmentations\n",
        "\n",
        "    # Apply augmentations\n",
        "    for aug_idx in range(num_augmentations_per_image):\n",
        "        seq_det = seq.to_deterministic()\n",
        "        image_aug = seq_det.augment_image(image_original)\n",
        "\n",
        "        # Wrap segmentations_ia in PolygonsOnImage for augmentation\n",
        "        polygons_on_image = ia.PolygonsOnImage(segmentations_ia, shape=image_original.shape)\n",
        "        polygons_aug_on_image = seq_det.augment_polygons([polygons_on_image])[0] # Augment and unwrap from batch\n",
        "        polygons_aug = polygons_aug_on_image.polygons\n",
        "\n",
        "        # Generate new filenames and IDs\n",
        "        # Ensure augmented IDs are distinct from original IDs and from each other\n",
        "        current_new_image_id = new_image_id\n",
        "        new_image_id += 1 # Increment for the next augmented image\n",
        "        new_image_filename = f\"{original_img_id}_aug{aug_idx}{original_img_path.suffix}\" # Use original COCO ID as base\n",
        "\n",
        "        # Save augmented image\n",
        "        cv2.imwrite(str(augmented_img_dir / new_image_filename), cv2.cvtColor(image_aug, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # Create new COCO image entry for augmented image\n",
        "        new_img_coco_info = {\n",
        "            \"id\": current_new_image_id,\n",
        "            \"file_name\": new_image_filename,\n",
        "            \"width\": original_width,\n",
        "            \"height\": original_height\n",
        "        }\n",
        "        augmented_coco_data[\"images\"].append(new_img_coco_info)\n",
        "\n",
        "        # Create new COCO annotation entries for augmented polygons\n",
        "        num_augmented_annotations = 0\n",
        "        for poly_ia in polygons_aug:\n",
        "            if poly_ia.is_valid and poly_ia.area > 0 and len(poly_ia.xx) >= 3:\n",
        "                # Convert augmented polygon back to COCO segmentation format\n",
        "                coco_segmentation = np.column_stack([poly_ia.xx, poly_ia.yy]).flatten().tolist()\n",
        "                # Ensure all elements in coco_segmentation are standard Python floats\n",
        "                coco_segmentation = [float(p) for p in coco_segmentation]\n",
        "\n",
        "                # Get bounding box from the polygon object\n",
        "                bbox_obj = poly_ia.to_bounding_box()\n",
        "                x_min, y_min, x_max, y_max = bbox_obj.x1, bbox_obj.y1, bbox_obj.x2, bbox_obj.y2\n",
        "                bbox_aug = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]\n",
        "\n",
        "                new_ann_coco_info = {\n",
        "                    \"id\": new_annotation_id,\n",
        "                    \"image_id\": current_new_image_id,\n",
        "                    \"category_id\": int(poly_ia.label), # Category ID is stored in label\n",
        "                    \"segmentation\": [coco_segmentation],\n",
        "                    \"area\": float(poly_ia.area), # Ensure JSON serializable float\n",
        "                    \"bbox\": bbox_aug,\n",
        "                    \"iscrowd\": 0\n",
        "                }\n",
        "                augmented_coco_data[\"annotations\"].append(new_ann_coco_info)\n",
        "                new_annotation_id += 1 # Increment for the next annotation\n",
        "                num_augmented_annotations += 1\n",
        "\n",
        "        augmented_dataset_info_list.append({\n",
        "            'id': current_new_image_id,\n",
        "            'aligned_image': new_image_filename,\n",
        "            'annotation_count': num_augmented_annotations\n",
        "        })\n",
        "        print(f\"    Generada aumentada {aug_idx+1}/{num_augmentations_per_image} (ID: {current_new_image_id}) con {num_augmented_annotations} anotaciones.\")\n",
        "\n",
        "\n",
        "# Save the augmented COCO JSON\n",
        "final_augmented_coco_path = augmented_annot_dir / 'filtered_coco.json'\n",
        "with open(final_augmented_coco_path, 'w') as f:\n",
        "    json.dump(augmented_coco_data, f, indent=4)\n",
        "print(f\"\\n‚úÖ 'filtered_coco.json' para el dataset aumentado creado en: {final_augmented_coco_path}\")\n",
        "\n",
        "# Save the augmented dataset_info.json\n",
        "final_dataset_info_path = augmented_annot_dir / 'dataset_info.json'\n",
        "with open(final_dataset_info_path, 'w') as f:\n",
        "    json.dump(augmented_dataset_info_list, f, indent=4)\n",
        "print(f\"‚úÖ 'dataset_info.json' para el dataset aumentado creado en: {final_dataset_info_path}\")\n",
        "\n",
        "print(f\"\\nüéâ Generaci√≥n del dataset aumentado completada.\")\n",
        "print(f\"Total de im√°genes en el dataset aumentado: {len(augmented_coco_data['images'])}\")\n",
        "print(f\"Total de anotaciones en el dataset aumentado: {len(augmented_coco_data['annotations'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2d0913"
      },
      "source": [
        "## Refactor and Visualize Model Predictions\n",
        "\n",
        "### Subtask:\n",
        "Refactor the code for visualizing model predictions. This step will load the trained segmentation model and allow for testing with a new image (e.g., from Google Drive). The image preprocessing, prediction, and visualization of the original image, probability map, and binary mask will be presented in a clean and concise manner.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "399e5985"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create a single code block that includes all the necessary steps to refactor and visualize model predictions. This includes importing libraries, defining the `SimpleUNet` model, loading the trained weights, preparing a test image for inference, running the prediction, and displaying the results with original image, probability map, and binary mask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7526aa"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. & 2. Definir la clase SimpleUNet (copiada del c√≥digo de entrenamiento)\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.enc2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(192, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec2 = nn.Sequential(\n",
        "            nn.Conv2d(67, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Salida (logits, sin sigmoid)\n",
        "        self.output = nn.Conv2d(64, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        b = self.bottleneck(e2)\n",
        "\n",
        "        d1 = self.up1(b)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = torch.cat([d2, x[:, :, :d2.shape[2], :d2.shape[3]]], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        out = self.output(d2)\n",
        "        return out # <-- Dejamos los logits crudos\n",
        "\n",
        "# 3. & 4. & 5. & 6. Cargar el modelo entrenado\n",
        "model_path = '/content/alopecia_dataset/alopecia_segmentation_model.pth'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SimpleUNet().to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval() # Poner el modelo en modo evaluaci√≥n\n",
        "\n",
        "print(f\"‚úÖ Modelo cargado exitosamente desde: {model_path} y puesto en modo evaluaci√≥n.\")\n",
        "\n",
        "# 7. & 8. & 9. & 10. & 11. Cargar y preprocesar una nueva imagen\n",
        "# Asegurarse de que `imagenes` y `images_dir` est√©n disponibles desde las celdas anteriores\n",
        "# Para esta demostraci√≥n, usaremos la primera imagen encontrada en el paso de extracci√≥n.\n",
        "if 'imagenes' not in locals() or not imagenes:\n",
        "    print(\"‚ö†Ô∏è `imagenes` no est√° definido o est√° vac√≠o. Re-listando im√°genes...\")\n",
        "    # Re-define paths if not globally available, assuming previous cell execution state\n",
        "    json_path = '/content/temp_extract/test-coco/result_coco.json'\n",
        "    images_dir = '/content/temp_extract/test-coco/images'\n",
        "\n",
        "    imagenes = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.PNG']:\n",
        "        imagenes.extend(list(Path(images_dir).rglob(ext)))\n",
        "    if not imagenes:\n",
        "        raise FileNotFoundError(\"No se encontraron im√°genes en el directorio extra√≠do para realizar la predicci√≥n.\")\n",
        "\n",
        "new_image_path = str(imagenes[0]) # Usar la ruta de la primera imagen encontrada\n",
        "img_size = 256 # El tama√±o que usamos para entrenar\n",
        "\n",
        "# Cargar la imagen\n",
        "new_img_bgr = cv2.imread(new_image_path)\n",
        "\n",
        "if new_img_bgr is None:\n",
        "    raise FileNotFoundError(f\"‚ùå No se pudo cargar la imagen: {new_image_path}\")\n",
        "\n",
        "# Redimensionar y normalizar\n",
        "new_img_resized = cv2.resize(new_img_bgr, (img_size, img_size))\n",
        "new_img_rgb = cv2.cvtColor(new_img_resized, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Convertir a tensor PyTorch y preprocesar\n",
        "input_tensor = torch.tensor(new_img_rgb).permute(2, 0, 1).float() / 255.0\n",
        "input_tensor = input_tensor.unsqueeze(0).to(device) # A√±adir dimensi√≥n de batch y mover a dispositivo\n",
        "\n",
        "print(f\"‚úÖ Imagen '{os.path.basename(new_image_path)}' cargada y preprocesada para predicci√≥n.\")\n",
        "\n",
        "# 12. & 13. Realizar la predicci√≥n y visualizar\n",
        "with torch.no_grad():\n",
        "    # Obtener predicci√≥n cruda (logits)\n",
        "    logits = model(input_tensor)\n",
        "\n",
        "    # Convertir a probabilidad (0 a 1) usando Sigmoid\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "# Procesar resultados para visualizar\n",
        "# Quitamos las dimensiones extra (batch, channels) para tener solo (256, 256)\n",
        "probs_map = probs.squeeze().cpu().numpy()\n",
        "mask_binary = (probs_map > 0.3).astype(np.uint8) # Umbral de 0.3 para la m√°scara binaria\n",
        "\n",
        "# Visualizar\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Imagen Original\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(new_img_rgb)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')\n",
        "\n",
        "# Mapa de Calor (Probabilidad)\n",
        "plt.subplot(1, 3, 2)\n",
        "im = plt.imshow(probs_map, cmap='jet', vmin=0, vmax=1)\n",
        "plt.title('Mapa de Calor (Confianza)')\n",
        "plt.axis('off')\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "\n",
        "# M√°scara Final\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(mask_binary, cmap='gray')\n",
        "plt.title('Predicci√≥n Final (> 0.3)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizaci√≥n de la predicci√≥n completada.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcb80a83"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution ended with a KeyboardInterrupt, which typically indicates a manual interruption rather than a code error. The code for visualizing model predictions already appears correct and complete based on the instructions. I will re-output the same code block to ensure it is executed without interruption.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e61101dc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. & 2. Definir la clase SimpleUNet (copiada del c√≥digo de entrenamiento)\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.enc2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(192, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec2 = nn.Sequential(\n",
        "            nn.Conv2d(67, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Salida (logits, sin sigmoid)\n",
        "        self.output = nn.Conv2d(64, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        b = self.bottleneck(e2)\n",
        "\n",
        "        d1 = self.up1(b)\n",
        "        d1 = torch.cat([d1, e1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        d2 = self.up2(d1)\n",
        "        d2 = torch.cat([d2, x[:, :, :d2.shape[2], :d2.shape[3]]], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        out = self.output(d2)\n",
        "        return out # <-- Dejamos los logits crudos\n",
        "\n",
        "# 3. & 4. & 5. & 6. Cargar el modelo entrenado\n",
        "model_path = '/content/alopecia_dataset/alopecia_segmentation_model.pth'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SimpleUNet().to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval() # Poner el modelo en modo evaluaci√≥n\n",
        "\n",
        "print(f\"‚úÖ Modelo cargado exitosamente desde: {model_path} y puesto en modo evaluaci√≥n.\")\n",
        "\n",
        "# 7. & 8. & 9. & 10. & 11. Cargar y preprocesar una nueva imagen\n",
        "# Asegurarse de que `imagenes` y `images_dir` est√©n disponibles desde las celdas anteriores\n",
        "# Para esta demostraci√≥n, usaremos la primera imagen encontrada en el paso de extracci√≥n.\n",
        "if 'imagenes' not in locals() or not imagenes:\n",
        "    print(\"‚ö†Ô∏è `imagenes` no est√° definido o est√° vac√≠o. Re-listando im√°genes...\")\n",
        "    # Re-define paths if not globally available, assuming previous cell execution state\n",
        "    json_path = '/content/temp_extract/test-coco/result_coco.json'\n",
        "    images_dir = '/content/temp_extract/test-coco/images'\n",
        "\n",
        "    imagenes = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.PNG']:\n",
        "        imagenes.extend(list(Path(images_dir).rglob(ext)))\n",
        "    if not imagenes:\n",
        "        raise FileNotFoundError(\"No se encontraron im√°genes en el directorio extra√≠do para realizar la predicci√≥n.\")\n",
        "\n",
        "new_image_path = str(imagenes[0]) # Usar la ruta de la primera imagen encontrada\n",
        "img_size = 256 # El tama√±o que usamos para entrenar\n",
        "\n",
        "# Cargar la imagen\n",
        "new_img_bgr = cv2.imread(new_image_path)\n",
        "\n",
        "if new_img_bgr is None:\n",
        "    raise FileNotFoundError(f\"‚ùå No se pudo cargar la imagen: {new_image_path}\")\n",
        "\n",
        "# Redimensionar y normalizar\n",
        "new_img_resized = cv2.resize(new_img_bgr, (img_size, img_size))\n",
        "new_img_rgb = cv2.cvtColor(new_img_resized, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Convertir a tensor PyTorch y preprocesar\n",
        "input_tensor = torch.tensor(new_img_rgb).permute(2, 0, 1).float() / 255.0\n",
        "input_tensor = input_tensor.unsqueeze(0).to(device) # A√±adir dimensi√≥n de batch y mover a dispositivo\n",
        "\n",
        "print(f\"‚úÖ Imagen '{os.path.basename(new_image_path)}' cargada y preprocesada para predicci√≥n.\")\n",
        "\n",
        "# 12. & 13. Realizar la predicci√≥n y visualizar\n",
        "with torch.no_grad():\n",
        "    # Obtener predicci√≥n cruda (logits)\n",
        "    logits = model(input_tensor)\n",
        "\n",
        "    # Convertir a probabilidad (0 a 1) usando Sigmoid\n",
        "    probs = torch.sigmoid(logits)\n",
        "\n",
        "# Procesar resultados para visualizar\n",
        "# Quitamos las dimensiones extra (batch, channels) para tener solo (256, 256)\n",
        "probs_map = probs.squeeze().cpu().numpy()\n",
        "mask_binary = (probs_map > 0.3).astype(np.uint8) # Umbral de 0.3 para la m√°scara binaria\n",
        "\n",
        "# Visualizar\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Imagen Original\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(new_img_rgb)\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')\n",
        "\n",
        "# Mapa de Calor (Probabilidad)\n",
        "plt.subplot(1, 3, 2)\n",
        "im = plt.imshow(probs_map, cmap='jet', vmin=0, vmax=1)\n",
        "plt.title('Mapa de Calor (Confianza)')\n",
        "plt.axis('off')\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "\n",
        "# M√°scara Final\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(mask_binary, cmap='gray')\n",
        "plt.title('Predicci√≥n Final (> 0.3)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualizaci√≥n de la predicci√≥n completada.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e74c35b"
      },
      "source": [
        "## Refactor and Train Segmentation Model with Augmented Data\n",
        "\n",
        "### Subtask:\n",
        "Consolidate and refactor the AlopeciaDataset class, the SimpleUNet model definition, and the training loop. This step will use the newly generated augmented dataset for training. The code will be optimized for clarity and robustness, ensuring that the model trains for 30 epochs, saves checkpoints, and outputs the final trained model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de47f6df"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires consolidating and refactoring the `AlopeciaDataset` class, `SimpleUNet` model, and the training loop into a single code block, pointing to the augmented dataset, and ensuring all specified training and visualization steps are included.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "679c306c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# ----------------------------\n",
        "# 1. DATASET PARA ENTRENAMIENTO\n",
        "# ----------------------------\n",
        "class AlopeciaDataset(Dataset):\n",
        "    def __init__(self, aligned_dir, img_size=256):\n",
        "        self.img_dir = os.path.join(aligned_dir, 'images')\n",
        "        self.annot_dir = os.path.join(aligned_dir, 'annotations')\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Cargar informaci√≥n del dataset\n",
        "        info_path = os.path.join(aligned_dir, 'annotations', 'dataset_info.json')\n",
        "        with open(info_path, 'r') as f:\n",
        "            self.dataset_info = json.load(f)\n",
        "\n",
        "        # Tambi√©n cargar COCO filtrado\n",
        "        coco_path = os.path.join(aligned_dir, 'annotations', 'filtered_coco.json')\n",
        "        with open(coco_path, 'r') as f:\n",
        "            self.coco_data = json.load(f)\n",
        "\n",
        "        print(f\"Dataset cargado: {len(self.dataset_info)} im√°genes, {sum(item['annotation_count'] for item in self.dataset_info)} anotaciones\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset_info[idx]\n",
        "        img_path = os.path.join(self.img_dir, item['aligned_image'])\n",
        "\n",
        "        # Cargar imagen\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise FileNotFoundError(f\"No se pudo cargar la imagen: {img_path}. Aseg√∫rate de que el archivo existe.\")\n",
        "\n",
        "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
        "        img = img / 255.0  # Normalizar [0, 1]\n",
        "        img = torch.tensor(img).permute(2, 0, 1).float()\n",
        "\n",
        "        # Crear m√°scara\n",
        "        mask = self._create_mask_from_coco(item['id'], self.img_size)\n",
        "        mask = torch.tensor(mask).unsqueeze(0).float()\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "    def _create_mask_from_coco(self, image_id, size):\n",
        "        mask = np.zeros((size, size), dtype=np.uint8)\n",
        "\n",
        "        # Get image info\n",
        "        image_info = None\n",
        "        for img_info in self.coco_data['images']:\n",
        "            if img_info['id'] == image_id:\n",
        "                image_info = img_info\n",
        "                break\n",
        "        if image_info is None:\n",
        "            raise ValueError(f\"Image info not found for image_id: {image_id}\")\n",
        "\n",
        "        img_width = image_info['width']\n",
        "        img_height = image_info['height']\n",
        "\n",
        "        # Buscar anotaciones\n",
        "        for ann in self.coco_data['annotations']:\n",
        "            if ann['image_id'] == image_id:\n",
        "                seg = ann['segmentation']\n",
        "                if isinstance(seg, list):\n",
        "                    for polygon in seg:\n",
        "                        pts = np.array(polygon).reshape(-1, 2)\n",
        "                        # Scale points to the target size, considering aspect ratio if necessary\n",
        "                        # For simplicity, assuming direct scaling of points for now, adjust as needed\n",
        "                        pts = (pts * (size / max(img_width, img_height))).astype(np.int32)\n",
        "                        # Ensure points are within bounds\n",
        "                        pts[:, 0] = np.clip(pts[:, 0], 0, size - 1)\n",
        "                        pts[:, 1] = np.clip(pts[:, 1], 0, size - 1)\n",
        "\n",
        "                        # Only fill if there are enough points to form a polygon\n",
        "                        if len(pts) >= 3:\n",
        "                            cv2.fillPoly(mask, [pts], 1)\n",
        "\n",
        "        return mask\n",
        "\n",
        "# ----------------------------\n",
        "# 2. MODELO MEJORADO (Sin Sigmoid al final)\n",
        "# ----------------------------\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.enc2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        # The `enc1` output size is (N, 64, H/2, W/2)\n",
        "        # The `up1` output size is (N, 128, H/2, W/2) given input (N, 256, H/4, W/4)\n",
        "        # Concatenation should be (128 + 64 = 192)\n",
        "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.dec1 = nn.Sequential(\n",
        "            nn.Conv2d(192, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # The `x` (original input) size is (N, 3, H, W)\n",
        "        # The `up2` output size is (N, 64, H, W) given input (N, 128, H/2, W/2)\n",
        "        # Concatenation should be (64 + 3 = 67)\n",
        "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.dec2 = nn.Sequential(\n",
        "            nn.Conv2d(67, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Salida (logits, sin sigmoid)\n",
        "        self.output = nn.Conv2d(64, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x) # Output size: N, 64, H/2, W/2\n",
        "        e2 = self.enc2(e1) # Output size: N, 128, H/4, W/4\n",
        "        b = self.bottleneck(e2) # Output size: N, 256, H/4, W/4\n",
        "\n",
        "        d1 = self.up1(b) # Output size: N, 128, H/2, W/2\n",
        "        d1 = torch.cat([d1, e1], dim=1) # Concatenate with e1, resulting in N, 192, H/2, W/2\n",
        "        d1 = self.dec1(d1) # Output size: N, 128, H/2, W/2\n",
        "\n",
        "        d2 = self.up2(d1) # Output size: N, 64, H, W\n",
        "        d2 = torch.cat([d2, x[:, :, :d2.shape[2], :d2.shape[3]]], dim=1) # Concatenate with original input, resulting in N, 67, H, W\n",
        "        d2 = self.dec2(d2) # Output size: N, 64, H, W\n",
        "\n",
        "        out = self.output(d2) # Output size: N, 1, H, W (logits)\n",
        "        return out # <-- Dejamos los logits crudos\n",
        "\n",
        "# ----------------------------\n",
        "# 3. ENTRENAMIENTO (Con pesos para clases desbalanceadas)\n",
        "# ----------------------------\n",
        "def train_model():\n",
        "    # a. Set the aligned_dir variable to point to the augmented dataset path\n",
        "    aligned_dir = '/content/alopecia_dataset/augmented_dataset'\n",
        "    # b. Set the device to 'cuda' if available, otherwise 'cpu'.\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "    # c. Instantiate the AlopeciaDataset with the aligned_dir and an img_size of 256.\n",
        "    dataset = AlopeciaDataset(aligned_dir, img_size=256)\n",
        "    # d. Create a DataLoader for the dataset with a batch_size of 2 and shuffle=True.\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # e. Instantiate the SimpleUNet model and move it to the device.\n",
        "    model = SimpleUNet().to(device)\n",
        "\n",
        "    # f. Define the pos_weight for BCEWithLogitsLoss as a torch.tensor([10.0]) and move it to the device.\n",
        "    pos_weight = torch.tensor([10.0]).to(device)\n",
        "    # g. Instantiate nn.BCEWithLogitsLoss with the defined pos_weight.\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    # h. Instantiate the Adam optimizer for the model parameters with a learning rate of 0.001.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # i. Set num_epochs to 30.\n",
        "    num_epochs = 30\n",
        "    train_losses = []\n",
        "\n",
        "    print(\"\\n=== COMIENZO DEL ENTRENAMIENTO CON DATA AUMENTADA ===\")\n",
        "\n",
        "    # j. Implement the training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(dataloader):\n",
        "            # iii. Moving images and masks to the device.\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # iv. Performing a forward pass (model(images)).\n",
        "            outputs = model(images)\n",
        "            # v. Calculating the loss (criterion(outputs, masks)).\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "            # vi. Zeroing gradients (optimizer.zero_grad()).\n",
        "            optimizer.zero_grad()\n",
        "            # vii. Performing backpropagation (loss.backward()).\n",
        "            loss.backward()\n",
        "            # viii. Updating model weights (optimizer.step()).\n",
        "            optimizer.step()\n",
        "\n",
        "            # ix. Accumulating and printing batch loss.\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 20 == 0: # Print less frequently for augmented dataset\n",
        "                 print(f\"  Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "        # x. Calculating and printing the average epoch loss.\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # xi. Saving a model checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            checkpoint_path = f'/content/alopecia_dataset/model_checkpoint_epoch_{epoch+1}.pth'\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"  Checkpoint guardado: {checkpoint_path}\")\n",
        "\n",
        "    # k. Save the final trained model\n",
        "    final_model_path = '/content/alopecia_dataset/alopecia_segmentation_model.pth'\n",
        "    torch.save(model.state_dict(), final_model_path)\n",
        "    print(f\"\\n‚úÖ Modelo final guardado: {final_model_path}\")\n",
        "\n",
        "    # l. Return the trained model and the list of epoch losses.\n",
        "    return model, train_losses\n",
        "\n",
        "# ----------------------------\n",
        "# 4. VISUALIZACI√ìN MEJORADA (Mapas de calor)\n",
        "# ----------------------------\n",
        "def visualize_predictions(model, dataset, num_samples=3):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
        "    if num_samples == 1: # Adjust axes indexing if only one sample\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        img, true_mask = dataset[i]\n",
        "        img_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(img_tensor)\n",
        "            probs = torch.sigmoid(logits) # Convertimos logits a prob (0-1)\n",
        "            pred_mask = (probs > 0.3).float() # Umbral bajado a 0.3\n",
        "\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "        true_mask_np = true_mask[0].cpu().numpy()\n",
        "        probs_np = probs[0][0].cpu().numpy()\n",
        "        pred_mask_np = pred_mask[0][0].cpu().numpy()\n",
        "\n",
        "        # Imagen\n",
        "        axes[i, 0].imshow(img_np)\n",
        "        axes[i, 0].set_title(f\"Imagen {i+1}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Real\n",
        "        axes[i, 1].imshow(true_mask_np, cmap='gray')\n",
        "        axes[i, 1].set_title(\"Real\")\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "        # Calor (Probabilidades)\n",
        "        im = axes[i, 2].imshow(probs_np, cmap='jet', vmin=0, vmax=1)\n",
        "        axes[i, 2].set_title(\"Confianza (Mapa Calor)\")\n",
        "        axes[i, 2].axis('off')\n",
        "        fig.colorbar(im, ax=axes[i, 2], fraction=0.046, pad=0.04)\n",
        "\n",
        "        # Predicci√≥n binaria\n",
        "        axes[i, 3].imshow(pred_mask_np, cmap='gray')\n",
        "        axes[i, 3].set_title(\"Predicci√≥n (>0.3)\")\n",
        "        axes[i, 3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ INICIANDO ENTRENAMIENTO DEL MODELO DE SEGMENTACI√ìN\")\n",
        "    try:\n",
        "        # a. Call train_model() to start the training process.\n",
        "        trained_model, losses = train_model()\n",
        "\n",
        "        # b. Plot the training loss curve using matplotlib.pyplot.\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('√âpoca')\n",
        "        plt.ylabel('P√©rdida')\n",
        "        plt.title('Curva de Aprendizaje')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # c. Re-instantiate AlopeciaDataset with the augmented dataset path for visualization purposes.\n",
        "        aligned_dir = '/content/alopecia_dataset/augmented_dataset'\n",
        "        dataset = AlopeciaDataset(aligned_dir, img_size=256)\n",
        "\n",
        "        print(\"\\nüéØ VISUALIZANDO PREDICCIONES DEL MODELO\")\n",
        "        # d. Call visualize_predictions() with the trained model and the dataset to display sample predictions.\n",
        "        visualize_predictions(trained_model, dataset, num_samples=min(3, len(dataset)))\n",
        "\n",
        "        print(\"\\n‚úÖ ¬°ENTRENAMIENTO COMPLETADO!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la ejecuci√≥n: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1b1898"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the entire process, confirming the successful generation of synthetic data, the training of the segmentation model with this augmented dataset, and any observed improvements or next steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "520fac90"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Initial Data Setup:** Google Drive was successfully mounted, and the COCO dataset (`test-coco-upload.zip`) containing 12 images and 19 annotations was extracted and loaded for initial processing.\n",
        "*   **Dependency Management:** All required libraries (`imgaug`, `opencv-python`, `scikit-image`) were successfully installed, and a critical `numpy` downgrade to version `1.26.4` was performed to ensure compatibility with `imgaug`, resolving previous `AttributeError` issues.\n",
        "*   **Synthetic Data Generation:** The original dataset was successfully augmented, expanding the dataset from 12 original images to **110 images** with a total of **180 annotations**. This process involved applying various geometric and photometric transformations using `imgaug`, with several technical issues related to JSON serialization and polygon handling successfully resolved.\n",
        "*   **Segmentation Model Training:** A `SimpleUNet` model was successfully trained for **30 epochs** using the newly generated augmented dataset. Training checkpoints were saved every 5 epochs, and the final trained model was stored as `/content/alopecia_dataset/alopecia_segmentation_model.pth`. The training loss curve indicated learning progression.\n",
        "*   **Model Prediction Visualization:** The trained segmentation model was successfully loaded, and its predictions for sample images were visualized. This included displaying the original image, a probability map, and a binary mask, confirming the model's ability to generate segmentations.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Leverage Augmented Data for Robustness:** The significant increase in dataset size (from 12 to 110 images) through augmentation provides a more robust foundation for training, likely leading to a more generalized segmentation model for alopecia detection.\n",
        "*   **Further Model Evaluation and Optimization:** The current setup provides a functional model. The next steps should involve a detailed quantitative evaluation (e.g., using metrics like IoU, Dice Score) on a dedicated validation set, followed by hyperparameter tuning and potentially exploring more advanced UNet architectures or transfer learning to further improve performance.\n"
      ]
    }
  ]
}